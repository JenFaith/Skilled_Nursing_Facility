{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Autograded Notebook (Canvas & CodeGrade)\n",
    "\n",
    "This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully.\n",
    "Instructions\n",
    "\n",
    "- **Download** this notebook as you would any other ipynb file \n",
    "- **Upload** to Google Colab or work locally (if you have that set-up)\n",
    "- **Delete** `raise NotImplementedError()`\n",
    "\n",
    "- **Write** your code in the `# YOUR CODE HERE` space\n",
    "\n",
    "\n",
    "- **Execute** the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas)\n",
    "\n",
    "- **Save** your notebook when you are finished\n",
    "- **Download** as a ipynb file (if working in Colab)\n",
    "- **Upload** your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more managable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "Successfully complete all these objectives to earn full credit. \n",
    "\n",
    "**Successful completion is defined as passing all the unit tests in each objective.**  \n",
    "\n",
    "Each unit test that you pass is 1 point. \n",
    "\n",
    "There are 5 total possible points in this sprint challenge. \n",
    "\n",
    "\n",
    "There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews\n",
    "\n",
    "____\n",
    "\n",
    "# Before you submit your notebook you must first\n",
    "\n",
    "1) Restart your notebook's Kernal\n",
    "\n",
    "2) Run all cells sequentially, from top to bottom, so that cell numbers are sequential numbers (i.e. 1,2,3,4,5...)\n",
    "- Easiest way to do this is to click on the **Cell** tab at the top of your notebook and select **Run All** from the drop down menu. \n",
    "\n",
    "3) Comment out the cell that generates a pyLDAvis visual in objective 4 (see instructions in that section). \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "I'm getting depreciation errors. I think I know how to fix it, but it involves updating my jupyter software package. I didn't want to risk giving myself any errors on the sprint challenge so I haven't updated it yet, and the warnings remain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Modeling\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# NLP Packages\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bec125eb29f89460cf0c19ba9aa9a2f",
     "grade": false,
     "grade_id": "cell-395851cd95d17235",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Load reviews from URL\n",
    "data_url = 'https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_4/unit1_nlp/review_sample.json'\n",
    "\n",
    "# Import data into a DataFrame named df\n",
    "df = pd.read_json(data_url, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "356579363f311da83f4ef7abaf3c9212",
     "grade": true,
     "grade_id": "cell-cb5006475e42b8f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Visible Testing\n",
    "assert isinstance(df, pd.DataFrame), 'df is not a DataFrame. Did you import the data into df?'\n",
    "assert df.shape[0] == 10000, 'DataFrame df has the wrong number of rows.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Optional: Consider using spaCy in your function. The spaCy library can be imported by running this cell.\n",
    "# A pre-trained model (en_core_web_sm) has been made available to you in the CodeGrade container.\n",
    "# If you DON'T need use the en_core_web_sm model, you can comment it out below.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Ensuring text column will be read as a string\n",
    "df['text']=df['text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Add custom stopwords\n",
    "\n",
    "nlp.Defaults.stop_words |= {'like', 'go', 'good', 'great', 'come', 'love', 's', 't'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4837ed2a1cc13057ba40203859d46ff6",
     "grade": false,
     "grade_id": "cell-3d570d5a1cd6cb64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "<>:11: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:11: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-10-850701c69527>:11: DeprecationWarning: invalid escape sequence \\s\n",
      "  doc = re.sub(\"\\s+\",\" \", doc)\n"
     ]
    }
   ],
   "source": [
    "def tokenize(doc, allowed = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"\n",
    "    Tokenize & Clean Document Strings\n",
    "    \"\"\"\n",
    "    lemmas = []\n",
    "    \n",
    "    # remove anything outside of letters and numbers\n",
    "    doc = re.sub('[^a-zA-Z]',' ', doc)\n",
    "    \n",
    "    # remove extra white space\n",
    "    doc = re.sub(\"\\s+\",\" \", doc)\n",
    "    \n",
    "    text = nlp(doc)\n",
    "    \n",
    "    # return list of tokenize, lemmatized, cleaned strings\n",
    "    # with no stopwords!\n",
    "    for token in text: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False) and\n",
    "            (token.pos_ != allowed)):\n",
    "            lemmas.append(token.lemma_.lower())\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2181ca9d36070260b1f75dcfd9e58965",
     "grade": true,
     "grade_id": "cell-02da164f6fbe730a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "'''Testing'''\n",
    "assert isinstance(tokenize(df.sample(n=1)[\"text\"].iloc[0]), list), \"Make sure your tokenizer function accepts a single document and returns a list of tokens!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Making new column with tokenize function applied to text column\n",
    "df['token'] = df['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "      <td>[beware, fake, fake, fake, small, business, lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "\n",
       "                  user_id                                              token  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  [beware, fake, fake, fake, small, business, lo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm it worked as expected\n",
    "# There are som extra spaces in the text that could be cleaned out later\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews (i.e. create a doc-term matrix).\n",
    "2. Write a fake review and query for the 10 most similiar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, use `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Make sure column is a string\n",
    "df['token_str'] = df['token'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    ['beware', 'fake', 'fake', 'fake', 'small', 'b...\n",
       "1    ['come', 'lunch', 'togo', 'service', 'quick', ...\n",
       "2    ['ve', 'vegas', 'dozen', 'time', 'step', 'foot...\n",
       "Name: token_str, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_str'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f570a35b1d17ce543ee41f516a0828c",
     "grade": false,
     "grade_id": "cell-0e96491cb529202c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaahhhs</th>\n",
       "      <th>aaasssk</th>\n",
       "      <th>aab</th>\n",
       "      <th>aamco</th>\n",
       "      <th>aand</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zupas</th>\n",
       "      <th>zur</th>\n",
       "      <th>zuzana</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>zxkxko</th>\n",
       "      <th>zyr</th>\n",
       "      <th>zyrtec</th>\n",
       "      <th>zzaplon</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.063886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 20717 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              aa  aaa  aaaahhhs  aaasssk  aab  aamco  aand  aaron   ab  ...  \\\n",
       "0  0.022947  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0  0.0  ...   \n",
       "1  0.063886  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0  0.0  ...   \n",
       "2  0.014944  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0  0.0  ...   \n",
       "3  0.049592  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0  0.0  ...   \n",
       "4  0.028545  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0  0.0  ...   \n",
       "\n",
       "   zuni  zupas  zur  zuzana  zuzu  zxkxko  zyr  zyrtec  zzaplon  zzzzzzzzz  \n",
       "0   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0        0.0  \n",
       "1   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0        0.0  \n",
       "2   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0        0.0  \n",
       "3   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0        0.0  \n",
       "4   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0        0.0  \n",
       "\n",
       "[5 rows x 20717 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector representation of the reviews \n",
    "# Name that doc-term matrix \"dtm\"\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize)\n",
    "\n",
    "# Grabs word counts per document\n",
    "fm = tfidf.fit_transform(df['token_str'])\n",
    "\n",
    "\n",
    "dtm = pd.DataFrame(data = fm.toarray(), columns=tfidf.get_feature_names())\n",
    "\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32b220e23c9aa1f602f08d1c2e879d0a",
     "grade": false,
     "grade_id": "cell-3d5bc610a8ec6b24",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Create and fit a NearestNeighbors model named \"nn\"\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
    "nn.fit(dtm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d270ed23df3c7d3c6cf08ab174ccaf9e",
     "grade": true,
     "grade_id": "cell-c43704dcff67e99b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "'''Testing.'''\n",
    "assert nn.__module__ == 'sklearn.neighbors._unsupervised', ' nn is not a NearestNeighbors instance.'\n",
    "assert nn.n_neighbors == 10, 'nn has the wrong value for n_neighbors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3da2ced9f187ed0aa1a890785e2ba00e",
     "grade": false,
     "grade_id": "cell-496203e8746296ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Create a fake review and find the 10 most similar reviews\n",
    "\n",
    "fake_review = [\"Quiche was delicious. Service was terrible\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "      <td>[beware, fake, fake, fake, small, business, lo...</td>\n",
       "      <td>['beware', 'fake', 'fake', 'fake', 'small', 'b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "\n",
       "                  user_id                                              token  \\\n",
       "0  n1LM36qNg4rqGXIcvVXv8w  [beware, fake, fake, fake, small, business, lo...   \n",
       "\n",
       "                                           token_str  \n",
       "0  ['beware', 'fake', 'fake', 'fake', 'small', 'b...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "df_analysis = df.copy()\n",
    "fake_review = [\"Quiche was delicious. Service was terrible\"]\n",
    "\n",
    "appendable = pd.DataFrame({'business_id': [0], 'cool': [5], 'date': [0], \n",
    "                           'funny': [3], 'review_id':[3934], 'stars':[2], \n",
    "                           'text':[fake_review],'useful': [3], 'user_id': [0], \n",
    "                           'token':[['quiche','delicious', 'service','terrible']]})\n",
    "\n",
    "df_analysis=df_analysis.append(appendable).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "my_rev = dtm.iloc[1000].values\n",
    "job = [my_rev]\n",
    "neigh_dist, neigh_index = nn.kneighbors(job)\n",
    "index_of_similar = neigh_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been a client since November 2013 and I can honestly say the I have seen excellent results with Debi. She takes the time to listen to my concerns and determines the perfect treatment plan for my particular issues. We have even discussed a very frustrating problem of adult acne, for which Debi has suggested great products that have helped. I am very happy with all aspects of the care I receive and plan to continue my treatments here with Debi.\n",
      "I just wanted to say a HUGE THANK YOU to Debi and her staff at FitForABride!  I was just married on November 8th, 2014 at the Mandalay Bay in Las Vegas.  I decided to go with the \"Ivory Sweet Package\" which included (1) Steaming of Wedding Gown and veil and (2) Full pick up and deliver to and from hotel bell desk for only $140.  I flew into Las Vegas a few days before the wedding.  The airline I flew in on pretty much smashed and wrinkled my dress in the first class closet no matter what care I took.  Debi was quick to take away all of my fears and quickly come and pick up my wedding dress and vail from the hotel.  I loved this service because I didn't have to worry or stress about my dress.  Which she brought it back the day before my wedding, it was BEAUTIFUL!  Very well packed in a huge bag, tissue paper everywhere preserving the steaming work, and even used cardboard in places to keep the dress in the perfect position.  It was 100% better than I ever imagined!  I was extremely happy with the steaming and my dress and vail were perfect for my big day!!  I just want to say THANK YOU so much for making me feel like a beautiful bride on our wedding day!  If I lived in Las Vegas, I would definitely use Debi and FitForABride again!\n",
      "Absolutely the best place in vegas for laser hair removal, they do have much more to offer but that's all I need. They have really good specials for many other treatments that my girl friend tried and really loved it. Today was my sixth treatment , hope to see good results...\n",
      "I am a loyal customer and I always will be, which is why it's so it's hard for me to not give them 5 stars. I love what Pure and Simple stands for, I love that I can find natural products at a fair price, I love their teas, I love the friendliness of the majority of their staff, and I love their facials. However, what I don't love is that they promote their products as helping with acne - If you want to know more about that skip to the large paragraph below but first I'd like to start with what i DO like about Pure and Simple:\n",
      "\n",
      "FACIALS: Recommend any facial - they are relaxing, and they always do a good job with extractions, and they aren't too expensive. My favourite is the hydrating facial since I have dry skin.\n",
      "\n",
      "Clarifying tea: Keeps me regular, I drink it every night before bed! \n",
      "\n",
      "Products: I like that they are all-natural and not too expensive. Currently I use their lip balms, lip gloss, cleanser, and moisturizer. They always have stuff on sale so sign up for their newsletter to keep up with the deals. \n",
      "\n",
      "Ok, so now for my not so great experience... When I first started going to Pure and Simple I was definitely getting a hard-sell from one of the employees that my skin care products are bad and I should switch to all natural skin care. I believe this to be very bad advice by their staff in regards to treating acne. I originally went to this location for a facial but mentioned that I was using acne products with chemicals and they explained why their natural approach was the way to go with acne prone skin. I won't get into the details but they basically told me a bunch of reasons why chemical based acne products (i.e. Retin A and benzol peroxide) are bad for my skin. They even had a book about the natural approach to skin care, which I bought and read. So I decided to use their products (cleanser, witch hazel, moisturizer). Within a week I basically began to form the worst acne of my life. **Please note that my skin was in fairly good condition when I first went to Pure and Simple - I got a pimple here and there but I've never had anything severe** After using their products for a few weeks i had cystic acne all over my face. I went back and they said that my skin was trying to \"release years of dirt that's been repressed in my pores from my chemical based acne products\". They also said that the acne could be a good thing because it's telling me there could be something wrong with me (i.e. maybe i have a soy allergy) and it's coming through my skin and that I should go search for what this problem might be. They also said to use lactic acid to help the acne. $50 later and this did not help. I also tried their clay mask and this did not help either. I tried to return the lactic acid but they told me that I need to use it longer even though my skin was clearly not reacting well to their products (at this point my skin was grossly cover in acne). I gave their skin regimen 3 months and my skin just got worse by the day with cystic acne all over. After my self confidence reached an all time low I went back on the benzol peroxide and within 3 months my skin was clear again.  I am now left with hyperpigmention all over my cheeks from this break out. It's been 1.5 years since this breakout and the scars haven't faded yet! I tried chemical peels and microdermabrasion but it's too expensive to keep it up on a regular basis so I just have to let them fade naturally :(\n",
      "\n",
      "The moral of my story is - your skin is precious - do your research before deciding to change your acne regimen. I don't think Pure + Simple and their employees should be promoting their products as being able to cure acne without independent research to back up these claims. I know it worked for the owner but that doesn't mean it's going to work for everyone.\n",
      "\n",
      "Despite all of this I will still go back because they are one of the few places in the city that offer truly all-natural products without breaking the bank. Plus they do great facials (using natural products) and they have an amazing clarifying tea that I have to drink every day!\n",
      "Paul truly goes Above and Beyond with his treatments. He not only offers excellent acupuncture and cupping, he truly cares about his patients. He provides nutritional advice in the way of healthy juicing as well as being a great listener who provides great spiritual insight into my daily problems. His clinic is so peaceful and inviting with soothing music and aromatherapy. When I went for my acupuncture treatment he put on some really interesting music that I enjoyed while I was getting my treatment.\n",
      "By far the worst hospital system and wellness plan for pets. The vet at the Parma location overcharged me for an adult cat when I told her that the cat was born in Oct. of 2014. The appointment was made in January. Among being overcharged on their wellness plan, I told them that I didn't want the plan. They insisted that I do the insurance plan and that I could easily terminate it. The cat I took in was my sister's not even mine. When I called to cancel, I was yelled at and treated poorly over the phone by two representatives, one who goes by \"Matt G.\" Their customer service is deplorable and all they could do was send me to a supervisor's voicemail and assure me that I would get a call back in 24 to 48 hours. I called over three times and never got a return phone call. I do NOT suggest this hospital or the wellness plan to anyone!\n",
      "Service is just awful!  When you go, plan on staying the day, and plan on going back another day to fix whatever damage is done on the first visit.  And don't expect them to be proactive about correcting the issue and doing so timely.  Your time means NOTHING to them!  Service advisor that I've worked with knows very little about the Honda vehicles, which contributes to the compounding service issues resulting from the service call.  Cannot escalate to the service manager because he's too busy putting out the many fires from the day.  So, when you have a problem, lob your complaint through the Berkshire Hathaway auto group in \n",
      "Scottsdale.\n",
      "\n",
      "OR, my advice is....just don't go here!\n",
      "After our house all of the sudden became infested with small black beetles, I called Insectek for information. I spoke with both Jenny and Ben who I learned were the owners of the company. They provided excellent information and advice, answered all of my questions, and were able to schedule a service appointment for the next day! \n",
      "\n",
      "Today a technician named Broc arrived early to my appointment and greeted me with respect and professionalism. He asked what sort of specific problems we had and the primary concern areas of the home. He and I spoke throughout the service, which lasted almost an hour! Broc explained what he was doing, how he was doing it, and the affects the treatment had on our pests. He was also able to locate the main \"nest\" of the beetle bugs. The treatment also flushed out countless crickets, a few scorpions, and even a cockroach! \n",
      "\n",
      "Overall I would not hesitate to recommend Insectek to anyone needing pest control. I agreed to future bi-monthly treatments of the home and feel that the pricing is both reasonable and competitive.\n",
      "\n",
      "5 out of 5\n",
      "I'm 36 and wanted to start some preventive care for my aging skin :/ A few friends and clients recommended Jane. I'm so grateful for finding her spa. I will start seeing her regularly! I can tell from my results that she really listened to my concerns. She is professional, knowledgeable, and most importantly made my skin look amazing after my first visit. Excited for my next visit. Highly recommend this company for any type of skincare!\n",
      "Had a problem with my security system on October 31.  I was told that someone would call to schedule an app't for repair. They say they called and left a voiceman but I didn't get it.  On November 2, I called and was given a app't for November 19th!  Do you think they need more repairmen?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Printing Reviews That Are Similar\n",
    "\n",
    "for i in range(len(index_of_similar)):\n",
    "    print(df_analysis.iloc[(index_of_similar[i])]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a pipeline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier.\n",
    "    - Use that pipeline to train a model to predict the `stars` feature (i.e. the labels). \n",
    "    - Use that Pipeline to predict a star rating for your fake review from Part 2. \n",
    "\n",
    "\n",
    "\n",
    "2. Create a parameter dict including `one parameter for the vectorizer` and `one parameter for the model`. \n",
    "    - Include 2 possible values for each parameter\n",
    "    - **Use `n_jobs` = 1** \n",
    "    - Due to limited computational resources on CodeGrader `DO NOT INCLUDE ADDITIONAL PARAMETERS OR VALUES PLEASE.`\n",
    "    \n",
    "    \n",
    "3. Train the entire pipeline with a GridSearch\n",
    "    - Name your GridSearch object as `gs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1d18da8521d51d8bfc4b5b9d005fa34",
     "grade": false,
     "grade_id": "cell-e2beb0252d274bba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Name the gridsearch instance \"gs\"\n",
    "\n",
    "# Assign X & Y\n",
    "X = df['token_str']\n",
    "Y = df['stars']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b5fec669aca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#Creating Pipeline\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "pipeline = Pipeline([(\"vectorize\", tfidf),\n",
    "                 (\"class\", rfc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Set Parameters for TFIDF and RFC\n",
    "parameters = {\n",
    "    # parameter for the vectorizer\n",
    "    \"vectorize__max_df\": [0.75, 1.0], \n",
    "    # parameter for the kneighbors model\n",
    "    \"class__n_estimators\":[10,20],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipeline,\n",
    "                 parameters, \n",
    "                 cv=3,\n",
    "                 n_jobs= 1,\n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vectorize',\n",
       "                                        TfidfVectorizer(stop_words='english')),\n",
       "                                       ('class', RandomForestClassifier())]),\n",
       "             n_jobs=1,\n",
       "             param_grid={'class__n_estimators': [10, 20],\n",
       "                         'vectorize__max_df': [0.75, 1.0]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5510439097325152"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5351515151515152"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actually slightly higher than training data which is interesting\n",
    "\n",
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Best Performing Nearest Neighbors Model for this Data so far\n",
    "best = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted number of stars is: 1 for the review ['Quiche was delicious. Service was terrible']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "gs_predicted = gs.predict(fake_review)\n",
    "print('Predicted number of stars is:', gs_predicted[0], 'for the review', fake_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted number of stars is: 1 for the review ['Terrible service. Disgusting food.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "review_2 = ['Terrible service. Disgusting food.']\n",
    "\n",
    "gs_predict2 = gs.predict(review_2)\n",
    "print('Predicted number of stars is:', gs_predict2[0], 'for the review', review_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9e2378efb868f104a4eb39e4f25563c",
     "grade": true,
     "grade_id": "cell-d07134c6fe5d056e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Visible Testing\n",
    "prediction = gs.predict([\"I wish dogs knew how to speak English.\"])[0]\n",
    "assert prediction in df.stars.values, 'You gs object should be able to accept raw text within a list. Did you include a vectorizer in your pipeline?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Set num_topics to `5`\n",
    "    - Name your LDA model `lda`\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "When you instantiate your LDA model, it should look like this: \n",
    "\n",
    "```python\n",
    "lda = LdaModel(corpus=corpus,\n",
    "               id2word=id2word,\n",
    "               random_state=723812,\n",
    "               num_topics = num_topics,\n",
    "               passes=1\n",
    "              )\n",
    "\n",
    "```\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about  pyLDAvis\n",
    "\n",
    "**pyLDAvis** is the Topic modeling package that we used in class to visualize the topics that LDA generates for us.\n",
    "\n",
    "You are welcomed to use pyLDAvis if you'd like for your visualization. However, **you MUST comment out the code that imports the package and the cell that generates the visualization before you submit your notebook to CodeGrade.** \n",
    "\n",
    "Although you should leave the print out of the visualization for graders to see (i.e. comment out the cell after you run it to create the viz). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "# Due to limited computationalresources on CodeGrader, use the non-multicore version of LDA \n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>token</th>\n",
       "      <th>token_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "      <td>[beware, fake, fake, fake, small, business, lo...</td>\n",
       "      <td>['beware', 'fake', 'fake', 'fake', 'small', 'b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "\n",
       "                  user_id                                              token  \\\n",
       "0  n1LM36qNg4rqGXIcvVXv8w  [beware, fake, fake, fake, small, business, lo...   \n",
       "\n",
       "                                           token_str  \n",
       "0  ['beware', 'fake', 'fake', 'fake', 'small', 'b...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimate a LDA topic model of the review tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9514841e71735eaa255bccc53b257896",
     "grade": false,
     "grade_id": "cell-66331a185ff52f15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Remember to read the LDA docs for more information on the various class attirbutes and methods available to you\n",
    "# in the LDA model: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "# don't change this value \n",
    "num_topics = 5\n",
    "\n",
    "# Creating library and corpus\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df['token'])\n",
    "\n",
    "# Term Document Frequency for each doc -> doc-term matrix \n",
    "corpus = [id2word.doc2bow(list_of_token) for list_of_token in df['token']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "lda = LdaModel(corpus=corpus,\n",
    "               id2word=id2word,\n",
    "               random_state=723812,\n",
    "               num_topics = num_topics,\n",
    "               passes=1\n",
    "              )\n",
    "lda.save('model5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"place\" + 0.009*\"t\" + 0.009*\"s\" + 0.008*\"food\" + 0.007*\"la\" + '\n",
      "  '0.007*\"try\" + 0.006*\"de\" + 0.005*\"d\" + 0.004*\"class\" + 0.004*\"donut\"'),\n",
      " (1,\n",
      "  '0.018*\"food\" + 0.018*\"t\" + 0.013*\"order\" + 0.011*\"s\" + 0.009*\"place\" + '\n",
      "  '0.009*\"service\" + 0.008*\"time\" + 0.007*\"chicken\" + 0.007*\"try\" + '\n",
      "  '0.006*\"restaurant\"'),\n",
      " (2,\n",
      "  '0.018*\"t\" + 0.017*\"s\" + 0.010*\"time\" + 0.010*\"place\" + 0.008*\"service\" + '\n",
      "  '0.008*\"go\" + 0.006*\"don\" + 0.005*\"food\" + 0.005*\"get\" + 0.005*\"work\"'),\n",
      " (3,\n",
      "  '0.013*\"place\" + 0.010*\"s\" + 0.009*\"time\" + 0.008*\"food\" + 0.008*\"t\" + '\n",
      "  '0.008*\"order\" + 0.008*\"get\" + 0.007*\"come\" + 0.006*\"delicious\" + '\n",
      "  '0.006*\"try\"'),\n",
      " (4,\n",
      "  '0.016*\"place\" + 0.010*\"friendly\" + 0.010*\"amazing\" + 0.010*\"good\" + '\n",
      "  '0.010*\"nice\" + 0.010*\"recommend\" + 0.009*\"service\" + 0.009*\"time\" + '\n",
      "  '0.009*\"staff\" + 0.008*\"food\"')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "pprint(lda.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6479db0fa59c99d3ae3201c1f10ebca1",
     "grade": true,
     "grade_id": "cell-5a3c181311134fa9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Visible Testing\n",
    "assert lda.get_topics().shape[0] == 5, 'Did your model complete its training? Did you set num_topics to 5?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x12ace4970>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "corpus_2 = [[(id2word[i], freq) for i, freq in doc] for doc in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "189591ed7b9e6e6146d59761fb418268",
     "grade": false,
     "grade_id": "cell-9b043e992fbd218c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jenniferquigley/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-gQtURTt4/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el440650775098406222080209\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el440650775098406222080209_data = {\"mdsDat\": {\"x\": [0.07442403550177831, 0.04027378592257391, 0.000455991747666421, 0.015948675220996234, -0.1311024883930152], \"y\": [-0.00027636557517581, 0.04704519550093838, 0.045498059801427626, -0.09514569709193252, 0.0028788073647422587], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [39.00009975099689, 33.608987687666875, 15.46134252881285, 8.616117701908523, 3.3134523306148607]}, \"tinfo\": {\"Term\": [\"place\", \"food\", \"try\", \"amazing\", \"friendly\", \"good\", \"s\", \"nice\", \"recommend\", \"staff\", \"t\", \"clean\", \"delicious\", \"service\", \"d\", \"order\", \"restaurant\", \"cream\", \"time\", \"definitely\", \"cheese\", \"chicken\", \"ice\", \"highly\", \"vegas\", \"sauce\", \"experience\", \"little\", \"taste\", \"awesome\", \"yuk\", \"brake\", \"tart\", \"michael\", \"freezer\", \"promotion\", \"removal\", \"physician\", \"in\", \"jewelry\", \"tim\", \"doughnut\", \"vet\", \"id\", \"cox\", \"iphone\", \"tire\", \"harness\", \"collection\", \"pant\", \"minor\", \"marry\", \"disgusted\", \"waiting\", \"attorney\", \"depot\", \"inspect\", \"warranty\", \"gym\", \"cha\", \"cat\", \"insurance\", \"quote\", \"mechanic\", \"tech\", \"car\", \"appointment\", \"pet\", \"repair\", \"estimate\", \"call\", \"company\", \"phone\", \"polish\", \"rent\", \"wear\", \"store\", \"dog\", \"fix\", \"dealership\", \"office\", \"doctor\", \"schedule\", \"work\", \"customer\", \"shop\", \"need\", \"tell\", \"money\", \"hour\", \"buy\", \"people\", \"s\", \"say\", \"purchase\", \"t\", \"don\", \"review\", \"business\", \"know\", \"year\", \"time\", \"go\", \"day\", \"find\", \"m\", \"place\", \"service\", \"price\", \"new\", \"look\", \"take\", \"want\", \"get\", \"ve\", \"come\", \"food\", \"staff\", \"ask\", \"order\", \"brewery\", \"cajun\", \"keg\", \"stretch\", \"adequate\", \"unorganized\", \"skinny\", \"owe\", \"ikea\", \"pakora\", \"tile\", \"nacho\", \"dunkin\", \"renovate\", \"dent\", \"daddy\", \"cirque\", \"tartar\", \"mashed\", \"lion\", \"nancy\", \"halal\", \"sore\", \"chilli\", \"fajita\", \"quinoa\", \"tilapia\", \"burro\", \"volcano\", \"cobbler\", \"attach\", \"kobe\", \"library\", \"taco\", \"margarita\", \"theater\", \"chicken\", \"rib\", \"cocktail\", \"prime\", \"waiter\", \"enchilada\", \"wing\", \"server\", \"bar\", \"burger\", \"table\", \"drink\", \"menu\", \"beer\", \"food\", \"order\", \"fry\", \"rice\", \"waitress\", \"bartender\", \"restaurant\", \"shrimp\", \"night\", \"meal\", \"dining\", \"grill\", \"plate\", \"portion\", \"t\", \"room\", \"meat\", \"dinner\", \"wait\", \"taste\", \"service\", \"try\", \"salad\", \"eat\", \"s\", \"place\", \"time\", \"come\", \"didn\", \"go\", \"ask\", \"get\", \"want\", \"ve\", \"look\", \"don\", \"peoria\", \"curly\", \"squid\", \"concoction\", \"knot\", \"heartbeat\", \"knife\", \"installer\", \"ink\", \"tai\", \"bumper\", \"ragu\", \"chive\", \"bento\", \"dash\", \"howard\", \"baba\", \"starbuck\", \"silky\", \"eating\", \"hahaha\", \"froyo\", \"london\", \"barbeque\", \"mercedes\", \"ganoush\", \"hawaii\", \"prickly\", \"takoyaki\", \"penne\", \"brulee\", \"scone\", \"asia\", \"vanilla\", \"ice\", \"cream\", \"pancake\", \"mayo\", \"marshmallow\", \"noodle\", \"skin\", \"milk\", \"chocolate\", \"tea\", \"mocha\", \"matcha\", \"prosciutto\", \"breakfast\", \"blueberry\", \"soup\", \"cheese\", \"egg\", \"tempura\", \"cup\", \"delicious\", \"sauce\", \"sashimi\", \"coffee\", \"homemade\", \"cheesecake\", \"flavor\", \"pork\", \"perfect\", \"topping\", \"crust\", \"dish\", \"pedicure\", \"place\", \"dessert\", \"crispy\", \"little\", \"get\", \"come\", \"taste\", \"order\", \"try\", \"time\", \"s\", \"food\", \"eat\", \"good\", \"go\", \"t\", \"find\", \"service\", \"amazing\", \"ve\", \"think\", \"definitely\", \"nice\", \"fav\", \"holy\", \"contractor\", \"appliance\", \"humor\", \"hakka\", \"nigiri\", \"spotless\", \"layout\", \"tostada\", \"halo\", \"precise\", \"musician\", \"broad\", \"pinball\", \"gay\", \"suzi\", \"teller\", \"efficiency\", \"pond\", \"mayonnaise\", \"enterprise\", \"pearson\", \"workmanship\", \"mikey\", \"optometrist\", \"implant\", \"giada\", \"donna\", \"jeff\", \"barber\", \"sammy\", \"frill\", \"snooze\", \"thread\", \"noodles\", \"tamale\", \"acai\", \"eyebrow\", \"utilize\", \"haircut\", \"floral\", \"informative\", \"japan\", \"highly\", \"installation\", \"amazing\", \"pastor\", \"friendly\", \"clean\", \"professional\", \"recommend\", \"poutine\", \"amazed\", \"hair\", \"good\", \"relax\", \"nice\", \"atmosphere\", \"awesome\", \"staff\", \"excellent\", \"vegas\", \"knowledgeable\", \"place\", \"comfortable\", \"super\", \"thank\", \"definitely\", \"helpful\", \"experience\", \"service\", \"look\", \"time\", \"cut\", \"roll\", \"fresh\", \"favorite\", \"food\", \"try\", \"s\", \"price\", \"restaurant\", \"t\", \"work\", \"come\", \"get\", \"une\", \"vous\", \"bouchon\", \"mojito\", \"avec\", \"possibility\", \"je\", \"mais\", \"sur\", \"en\", \"ce\", \"les\", \"pas\", \"grub\", \"ou\", \"comme\", \"sont\", \"rick\", \"des\", \"nous\", \"jamba\", \"guys\", \"fluke\", \"aussi\", \"wth\", \"pe\", \"bonne\", \"asado\", \"si\", \"bobbie\", \"et\", \"tr\", \"de\", \"est\", \"bon\", \"le\", \"cupcake\", \"ai\", \"shawarma\", \"un\", \"que\", \"la\", \"j\", \"l\", \"yoga\", \"donut\", \"y\", \"class\", \"frost\", \"farmer\", \"au\", \"pour\", \"cake\", \"roof\", \"place\", \"d\", \"try\", \"s\", \"t\", \"food\", \"e\", \"delicious\", \"well\", \"service\", \"ve\", \"order\", \"restaurant\", \"little\", \"friend\", \"m\", \"time\", \"year\", \"taste\", \"go\", \"tasty\"], \"Freq\": [5299.0, 5072.0, 2575.0, 1366.0, 1403.0, 1633.0, 6327.0, 1747.0, 1394.0, 1609.0, 7286.0, 864.0, 1288.0, 3805.0, 815.0, 3571.0, 1728.0, 608.0, 4347.0, 1419.0, 909.0, 1481.0, 530.0, 540.0, 903.0, 961.0, 1441.0, 1450.0, 1214.0, 679.0, 53.948287693326755, 36.56240059293293, 38.43674891449676, 41.72310982581454, 23.224926026186257, 22.66619194201253, 22.84112824642554, 22.51964243056721, 21.260862930588154, 34.67248922081087, 22.412621788918067, 18.73604636231292, 78.64599002898987, 20.083103576647986, 25.502983881873007, 23.480932509663702, 135.21726464594389, 26.027327210013173, 36.53153299578178, 27.851728674016524, 33.22399143698162, 14.892859570823598, 19.830313104868324, 58.92264336332634, 21.32606152210346, 26.81405484258201, 27.942683155239404, 68.2948074722501, 119.71815629435028, 17.2291794949558, 68.62135079310767, 99.45112297694686, 100.5837928462236, 53.38080199910076, 91.12653367929124, 581.4295648929725, 333.66822382783647, 87.66618848085214, 178.9225245060925, 48.79435237618196, 664.8761125388802, 338.53870537757143, 322.01909008153166, 86.1291572583141, 90.56202772969795, 95.29595803425978, 637.8971933178894, 317.46890884291076, 297.95589545742985, 70.73401197458607, 265.6078321987332, 125.72857917974176, 133.61621894682852, 991.3946614895763, 756.9737038557, 395.5969458729681, 895.742190352155, 970.3943107943775, 353.609230832313, 680.680855965877, 353.2159712514985, 914.0241244663296, 3206.921430858935, 933.9451849712873, 225.33429308718468, 3428.35648348834, 1068.6710030033894, 591.8640524372512, 364.78838769196165, 937.2397151638322, 599.4592436107795, 2003.8565622702852, 1480.782049421137, 874.1828039848231, 872.546968655276, 916.8578171382186, 1983.5768650381929, 1514.2108705054907, 880.6178960726236, 643.0939124311036, 919.9927753346128, 740.9107010692699, 826.0148317331632, 1044.6909966231003, 840.3844945660154, 858.7203309263814, 1045.1962189966432, 699.6106399317591, 677.0100329474037, 751.2500146805044, 41.41808034082256, 35.67980491114284, 30.26101535730727, 29.208357112321554, 23.61334442303356, 18.54934262165602, 18.49414833219464, 19.878632240463112, 20.935456047004827, 18.78861568723854, 29.50652652336591, 20.68028845370245, 21.59482683389203, 15.858916839540973, 18.420550357031857, 14.96671894805296, 30.579844899703264, 16.806895887989626, 27.659422151560385, 12.399536538472098, 13.057000604831078, 20.167790241171833, 15.385361477497224, 16.2402369914096, 39.6926491715685, 13.536998334790216, 11.02592165582775, 12.18111403675628, 10.734338335351662, 10.764563209252525, 31.122278145451432, 18.80466830987657, 36.940635837316414, 440.73352520193947, 112.5084305636909, 65.04212251998555, 1140.5415811391526, 181.73744787211848, 182.2050830585523, 82.29908829084941, 273.38907700597116, 66.68837927514507, 176.63366483811225, 627.7558658137145, 781.3066952851074, 525.0959629681191, 780.3925425607031, 967.3004416147442, 899.307055860685, 450.9902635802576, 2941.581884545316, 2105.3734424680138, 686.2711450337034, 388.0677962802459, 349.67709948121774, 195.83432139427273, 1063.0586676908742, 272.7155543285303, 674.9618969238076, 571.4970705699973, 207.15750101998782, 204.44236931384253, 286.27871856996626, 326.6483613210427, 2916.168653432703, 643.7622001760043, 419.2922516503855, 428.3930383130923, 827.4948634044772, 629.0499829062953, 1488.580817268139, 1108.4529502017706, 453.42898788038224, 723.0311837761648, 1876.3109611292166, 1491.0887200718505, 1279.4131537358971, 898.9290077945469, 672.6560490070265, 974.5995594269762, 641.7767579600458, 855.9086402488363, 679.8016326999409, 676.4491006097788, 606.6656971216385, 583.8034472825608, 24.817767401843412, 21.472519439754215, 23.496566109093468, 19.179903132272266, 17.78933898102531, 18.476021913487592, 22.93138195634848, 18.67870537624644, 14.292929685798807, 13.493370808915744, 14.831416658897957, 14.240914980154578, 16.05117704388807, 26.864981673645786, 13.932807025022877, 11.869938312553117, 12.286730357093722, 16.17018980233649, 12.926080811026777, 10.612020366181227, 10.354060396822534, 14.305488721437706, 10.177887882506313, 14.415289096106081, 15.229675943819805, 9.462189247016232, 14.372487876201681, 11.260231678723773, 9.86539412744105, 17.480400148217754, 28.194652629282505, 20.867182076461702, 22.425326761602268, 60.28189003276502, 405.0828843642092, 454.64910589758915, 123.18090810146305, 43.04232870476833, 27.811886898899825, 210.43831518971703, 101.83471292331282, 95.7991645918262, 197.54309366043384, 229.17191359055906, 40.032343190766326, 32.046070389935274, 24.17320027030094, 269.95877745994596, 38.67347691265729, 275.19446167168985, 413.01188170339526, 224.64603252946998, 46.525381145473546, 106.73329441413306, 490.17269248228337, 388.85505435153976, 68.20052454215836, 271.50210298993414, 72.46758610525858, 49.703327369494055, 291.66442310340494, 169.16440272790905, 221.12517405255352, 91.08429302023313, 85.54956337060722, 299.0986493160156, 86.95014543823508, 973.0292888406188, 204.6808133505009, 106.84328763176333, 386.7800552657865, 577.3976578745463, 518.9206192829837, 314.36334884759634, 595.053134087097, 478.43214849128776, 652.7006878799685, 775.5774903442292, 616.457248271405, 336.0815127541822, 324.94992055456504, 430.3996433985942, 595.9180579123869, 287.9039015973551, 356.82300529787454, 266.2149175858046, 286.49228230054774, 260.23025334922556, 252.37906318634825, 253.691047687479, 22.49473765718214, 36.14624380550498, 23.71158617189549, 14.082470209006352, 12.347207907441637, 11.86783779564436, 32.563564741268635, 14.103420413758558, 26.698843664770685, 15.99410085989843, 7.9613701114266044, 7.905646595198893, 8.803376010300626, 8.04552710111066, 8.989514489702078, 7.738470519729419, 9.360401154696826, 7.157693703994088, 7.5192978242873965, 7.981579726809271, 8.194275028661005, 7.081712673568764, 7.491552742774048, 7.080544679388946, 8.22214108538646, 6.071433325093126, 11.957462619280626, 6.971025267191383, 6.339617561796814, 7.521319399913034, 46.335829942942155, 9.081265283817066, 10.868454813481396, 9.695961829582084, 11.21577403334834, 18.646893209758396, 27.7349343336281, 10.022492349675423, 42.21657115924277, 10.657157174486267, 50.84580550102361, 10.474924290764967, 34.41054058920783, 13.941925260559064, 228.0593945113573, 18.99225594726104, 439.4061801469679, 18.325891318267008, 440.6975518486876, 300.0250697361706, 145.73134370097821, 419.454294777599, 36.56397796786567, 19.45457530911255, 148.82843686104707, 431.6642774929215, 55.84962521023901, 430.8649617199111, 157.96484841896304, 205.29722917311267, 363.293302706721, 192.4490475601587, 221.46444986014538, 70.77304385966637, 658.3470944516451, 90.73633160334175, 170.90287347065737, 153.2849327781548, 261.195843817468, 118.79809060491395, 239.18321878628242, 387.6789578477683, 255.69411224237254, 368.8145082205286, 116.85090306706084, 130.77394391278406, 166.5773402826584, 132.86790324502732, 343.54228777278604, 243.62131173305323, 324.79014244397183, 188.49214365197835, 177.11122801462096, 199.2126746431614, 151.55890512247657, 158.1142141184087, 157.54952884814548, 23.248617538924982, 18.535195725806645, 12.420065273818427, 12.217642608281151, 12.448459745318994, 12.032339374761067, 15.40460865500158, 11.008547652877887, 9.959393704529466, 33.27762913344148, 13.950344880684558, 25.845560656125443, 18.011378702140508, 19.242834811586203, 9.515541044330709, 9.204998533854749, 13.217176369881734, 7.012212588692007, 23.408293106461155, 9.418243763681657, 7.720371218013443, 8.97291748907321, 6.992455764731318, 7.401606222518796, 6.149801954099293, 6.297768427138468, 6.518428015620286, 7.81801375896169, 7.132708433110536, 6.714160849587416, 46.13626000345159, 18.27720551690396, 101.34811063534694, 25.70902083482141, 14.133893001657716, 58.442916479075016, 44.89152813957398, 17.804458678779813, 29.18545481670998, 42.01675580716806, 20.107670769894867, 111.64213099970459, 29.92964520759759, 53.132153729493204, 21.805595742229578, 66.86694660702491, 35.55300691545658, 69.23097929902607, 17.05773480465708, 17.067194275617748, 18.862466593706458, 33.98915440491429, 54.49105292513696, 22.395408020458927, 193.34393875508908, 81.69855068968921, 108.383432993094, 144.17813156423225, 146.45222843526045, 125.29783612541777, 34.41202128451965, 51.677650965146924, 48.4064159005585, 58.66292711044179, 49.531286235450985, 52.40477765687001, 46.42011592374283, 43.494008910900874, 41.447653479142595, 44.00963792834701, 43.103690017257236, 36.953469594450574, 37.190717735912244, 37.35035173996507, 35.78674861032288], \"Total\": [5299.0, 5072.0, 2575.0, 1366.0, 1403.0, 1633.0, 6327.0, 1747.0, 1394.0, 1609.0, 7286.0, 864.0, 1288.0, 3805.0, 815.0, 3571.0, 1728.0, 608.0, 4347.0, 1419.0, 909.0, 1481.0, 530.0, 540.0, 903.0, 961.0, 1441.0, 1450.0, 1214.0, 679.0, 54.77502283730683, 37.3992056970471, 39.38122935343253, 42.78376085326214, 24.04892121026844, 23.478733016389093, 23.66128066653634, 23.343713365678138, 22.063872685540616, 35.985666126845786, 23.343875296030053, 19.525307086519927, 82.10637506404035, 20.985005770015455, 26.709419407652835, 24.615793997611053, 141.85765607977473, 27.32411339077655, 38.3844063149759, 29.297286111820906, 34.95514914014267, 15.674061042770374, 20.8936814002094, 62.08630072298964, 22.476011283554143, 28.27503829189026, 29.5373250368872, 72.2516352342628, 126.7257666988511, 18.24051766114914, 73.11112180421007, 107.25557688349575, 109.08551041545778, 57.294805078468954, 99.07553380171221, 679.5255104871865, 382.63191955098546, 96.01413556905287, 201.97125034604107, 52.5027378238693, 801.0590806351979, 396.3758951545873, 381.1738847544027, 95.76328411591133, 101.00630962015882, 106.63077237495317, 809.4346689198823, 385.59824778353254, 366.50121154238263, 78.16448074445819, 324.6015814425126, 147.04261791638083, 157.72599355747795, 1464.188119286392, 1129.2388903639358, 543.2402301079751, 1376.472854577421, 1531.6076687637403, 483.43528422604726, 1031.0650479231392, 484.9735695050836, 1466.1792389696507, 6327.778156340583, 1531.5431717681276, 290.5677930156446, 7286.108097911852, 1825.8940375064217, 929.9501662936063, 522.8682658141761, 1669.2743489556108, 968.9087939568059, 4347.888602123937, 3035.5343430740763, 1575.6053590160468, 1589.9364809965973, 1781.3139028366838, 5299.385907157396, 3805.9565780297144, 1769.674889242743, 1135.7567569889702, 1976.33943048247, 1457.71667627061, 1793.370331373402, 2662.8325343886, 1983.0199678728654, 2463.439996547135, 5072.075475711568, 1609.230888318616, 1499.877370708557, 3571.5443197028953, 42.19475573544192, 36.45038620856629, 31.075877293524503, 30.04163670980815, 24.520668895071076, 19.321410832844204, 19.317934164075158, 20.793623086024283, 21.91089351043478, 19.679102651786263, 30.950433515351985, 21.707449865290318, 22.6709572349968, 16.65561678282374, 19.364051193700195, 15.73726172094468, 32.21100713329108, 17.74227194477252, 29.221710462229524, 13.154926458075613, 13.858082105162683, 21.46296671348518, 16.37711545906663, 17.304691317355303, 42.45097110171073, 14.480074166358966, 11.799004713586646, 13.041475774291412, 11.494541452339082, 11.534302451227049, 33.60821265285395, 20.236399423189912, 40.05680410417528, 524.6180888493687, 129.17166520117172, 73.33717942811768, 1481.7218201882743, 221.98578415982624, 225.00824297154358, 96.61655109174814, 352.1319438182652, 77.42709000172138, 221.03537332565486, 883.7550609950144, 1125.1939735902731, 732.3937685663375, 1150.848957775271, 1464.8018441644742, 1355.0069658534578, 640.3985160269337, 5072.075475711568, 3571.5443197028953, 1035.4284805133525, 554.1126090815832, 494.0585805907329, 257.98226530778464, 1728.4468413682387, 375.13691392192885, 1083.2934403742688, 928.3097177332928, 292.97497979063365, 288.5899893378048, 431.54155099048535, 508.34454095529384, 7286.108097911852, 1156.7877625789033, 695.6829457841964, 715.6945013144449, 1720.9375371851008, 1214.6854523128231, 3805.9565780297144, 2575.269234874043, 790.3580443339936, 1587.8211700654488, 6327.778156340583, 5299.385907157396, 4347.888602123937, 2463.439996547135, 1539.0857080853057, 3035.5343430740763, 1499.877370708557, 2662.8325343886, 1793.370331373402, 1983.0199678728654, 1976.33943048247, 1825.8940375064217, 25.711223506328423, 22.247423535268272, 24.38436637085345, 19.95552762597553, 18.537692983259756, 19.320109776483402, 24.099645274233566, 19.6446842643448, 15.088164930504057, 14.253245142315132, 15.669459557600705, 15.106959824871101, 17.049191280779947, 28.627031860496015, 14.848853483552846, 12.659990614804517, 13.107952991302042, 17.28361850158359, 13.819132810257596, 11.375645139128714, 11.109393022420278, 15.35513183863125, 10.932473480503944, 15.54397175285828, 16.446327039228777, 10.237378145296262, 15.585561269865295, 12.230221593327226, 10.721095464663929, 19.039024477497875, 30.80781865323498, 22.82005761607061, 24.634836321911468, 70.86776156180684, 530.2868371768745, 608.0402476918524, 158.23234350168124, 51.27320209243432, 32.82311638213146, 327.47190365219166, 143.99974075331318, 135.09909021782306, 310.65358404195496, 388.9847977536112, 51.010769699877564, 39.36712452534937, 28.19641730371522, 505.3970226992926, 49.40622641212007, 550.1237206664543, 909.7569910951477, 437.27930537537566, 62.738045700168215, 181.4887957600916, 1288.251309561189, 961.9686806115109, 104.14945585566501, 648.9995543266904, 114.66857262520998, 70.47275583153086, 768.0417411958128, 393.3426845534757, 576.3821110134425, 166.21771813468828, 152.18613045955826, 926.0552901088863, 156.88963554367126, 5299.385907157396, 550.6779450292576, 212.60641153686862, 1450.1859590948745, 2662.8325343886, 2463.439996547135, 1214.6854523128231, 3571.5443197028953, 2575.269234874043, 4347.888602123937, 6327.778156340583, 5072.075475711568, 1587.8211700654488, 1633.1933246293527, 3035.5343430740763, 7286.108097911852, 1589.9364809965973, 3805.9565780297144, 1366.1705468002303, 1983.0199678728654, 1468.3646043179467, 1419.9116512432795, 1747.8828035112308, 23.300236124020948, 37.51903216915553, 24.825030546126104, 14.882256759616658, 13.153866381668283, 12.655452333264083, 34.96713340598329, 15.288355996404942, 29.179442256410592, 17.506773461051438, 8.748266269621732, 8.687508913954531, 9.730920364483008, 8.89361415833463, 9.945501605704283, 8.561909064271386, 10.385527682525673, 7.95784816766656, 8.37009740674263, 8.900210609081062, 9.141081443159115, 7.905024587731086, 8.373197054021691, 7.980015450796667, 9.290194530097349, 6.869273364395967, 13.52937564137963, 7.899110650290002, 7.189188371872361, 8.541698309930247, 53.69373056396215, 10.3513863503925, 12.435326960835724, 11.108989478390935, 13.191813466814128, 23.10010251139045, 36.489679202577506, 11.660084263081725, 61.603581177678315, 12.55581336223022, 81.77132699455551, 12.327153560873791, 52.36528363171047, 17.40131437774461, 540.2046368603482, 25.69455166581536, 1366.1705468002303, 24.86139267122707, 1403.2878875916572, 864.4260783748797, 355.9423314055889, 1394.644408565306, 60.678756632753334, 26.99319890685544, 392.73568777879535, 1633.1933246293527, 108.17507973478295, 1747.8828035112308, 454.3090057525329, 679.7061622246403, 1609.230888318616, 674.8257849768169, 903.0023976556676, 172.10764165336602, 5299.385907157396, 258.8499813069693, 688.3164692254568, 593.573220061156, 1419.9116512432795, 406.0536517499435, 1441.9248958400226, 3805.9565780297144, 1976.33943048247, 4347.888602123937, 456.62719789333295, 571.3671404826985, 945.6698964111871, 626.8148305585362, 5072.075475711568, 2575.269234874043, 6327.778156340583, 1769.674889242743, 1728.4468413682387, 7286.108097911852, 1464.188119286392, 2463.439996547135, 2662.8325343886, 24.663255165356826, 19.725846992346977, 13.247779249097073, 13.032776868346257, 13.340173046516695, 12.906660984887719, 16.57415105915784, 11.934361034481432, 10.857244951933827, 36.4229744220265, 15.303590934164847, 28.359538428427605, 19.812829836761065, 21.190508680667246, 10.47968775886123, 10.209423426646486, 14.735401242431609, 7.8308459013773835, 26.237523707174248, 10.565497441923242, 8.671010313438927, 10.111985858878299, 7.889383357580699, 8.393140592771413, 6.979719756313865, 7.161583524559834, 7.414908754083907, 8.896631919504081, 8.165955043726424, 7.687913403331156, 53.37042779395639, 20.980195193323837, 121.03358890821727, 29.93555113593154, 16.266242135066335, 69.25687833960126, 52.93647290704307, 20.704148456520272, 34.857419784571235, 51.39815841432206, 24.29714426824886, 159.83547476599523, 39.625443359487676, 84.6938217008206, 28.643714489440736, 142.94472895660428, 59.53466109572179, 199.59515020276993, 22.462128829945073, 22.786088335148087, 27.15963447728648, 77.72679821770393, 218.5216781064196, 39.07989859903314, 5299.385907157396, 815.1123794132883, 2575.269234874043, 6327.778156340583, 7286.108097911852, 5072.075475711568, 152.93831422236084, 1288.251309561189, 1186.968356218622, 3805.9565780297144, 1983.0199678728654, 3571.5443197028953, 1728.4468413682387, 1450.1859590948745, 1052.8555224370284, 1781.3139028366838, 4347.888602123937, 968.9087939568059, 1214.6854523128231, 3035.5343430740763, 530.847883075849], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.1721, -8.5611, -8.5111, -8.429, -9.0149, -9.0392, -9.0315, -9.0457, -9.1032, -8.6142, -9.0505, -9.2297, -7.7951, -9.1602, -8.9213, -9.0039, -7.2532, -8.901, -8.5619, -8.8332, -8.6568, -9.4592, -9.1729, -8.0839, -9.1002, -8.8712, -8.8299, -7.9363, -7.375, -9.3135, -7.9315, -7.5604, -7.5491, -8.1826, -7.6479, -5.7946, -6.35, -7.6866, -6.9731, -8.2725, -5.6605, -6.3355, -6.3855, -7.7043, -7.6541, -7.6031, -5.7019, -6.3997, -6.4632, -7.9012, -6.5781, -7.326, -7.2651, -5.261, -5.5308, -6.1797, -5.3624, -5.2824, -6.2919, -5.637, -6.293, -5.3422, -4.087, -5.3207, -6.7425, -4.0203, -5.1859, -5.7768, -6.2608, -5.3172, -5.7641, -4.5573, -4.8598, -5.3868, -5.3887, -5.3391, -4.5674, -4.8374, -5.3795, -5.6938, -5.3357, -5.5522, -5.4435, -5.2086, -5.4262, -5.4047, -5.2081, -5.6096, -5.6424, -5.5384, -8.2876, -8.4367, -8.6015, -8.6369, -8.8495, -9.0909, -9.0939, -9.0217, -8.9699, -9.0781, -8.6267, -8.9821, -8.9389, -9.2476, -9.0979, -9.3055, -8.591, -9.1895, -8.6914, -9.4937, -9.442, -9.0072, -9.2779, -9.2238, -8.3302, -9.4059, -9.6111, -9.5114, -9.6379, -9.6351, -8.5734, -9.0772, -8.402, -5.9229, -7.2883, -7.8363, -4.9721, -6.8088, -6.8062, -7.601, -6.4004, -7.8113, -6.8373, -5.5692, -5.3504, -5.7477, -5.3515, -5.1368, -5.2097, -5.8999, -4.0246, -4.3591, -5.4801, -6.0501, -6.1543, -6.7341, -5.0424, -6.4029, -5.4967, -5.6631, -6.6778, -6.691, -6.3544, -6.2224, -4.0333, -5.544, -5.9728, -5.9513, -5.2929, -5.5671, -4.7058, -5.0006, -5.8945, -5.4279, -4.4743, -4.7041, -4.8572, -5.2101, -5.5001, -5.1293, -5.5471, -5.2592, -5.4895, -5.4945, -5.6034, -5.6418, -8.0233, -8.1681, -8.078, -8.281, -8.3563, -8.3184, -8.1024, -8.3075, -8.5751, -8.6327, -8.5381, -8.5788, -8.4591, -7.9441, -8.6006, -8.7609, -8.7264, -8.4517, -8.6756, -8.8729, -8.8975, -8.5742, -8.9147, -8.5666, -8.5116, -8.9876, -8.5696, -8.8136, -8.9458, -8.3738, -7.8957, -8.1967, -8.1247, -7.1358, -5.2308, -5.1154, -6.4212, -7.4727, -7.9094, -5.8857, -6.6115, -6.6726, -5.9489, -5.8004, -7.5452, -7.7677, -8.0496, -5.6366, -7.5797, -5.6174, -5.2114, -5.8204, -7.3949, -6.5645, -5.0401, -5.2717, -7.0124, -5.6309, -6.9517, -7.3288, -5.5593, -6.104, -5.8361, -6.7231, -6.7858, -5.5341, -6.7695, -4.3545, -5.9134, -6.5635, -5.277, -4.8763, -4.9831, -5.4843, -4.8462, -5.0644, -4.7538, -4.5813, -4.8109, -5.4175, -5.4512, -5.1702, -4.8448, -5.5723, -5.3576, -5.6506, -5.5772, -5.6733, -5.7039, -5.6988, -7.5369, -7.0626, -7.4842, -8.0052, -8.1367, -8.1763, -7.167, -8.0038, -7.3655, -7.8779, -8.5756, -8.5826, -8.475, -8.5651, -8.4541, -8.604, -8.4137, -8.682, -8.6327, -8.573, -8.5467, -8.6927, -8.6364, -8.6928, -8.5433, -8.8466, -8.1688, -8.7084, -8.8034, -8.6324, -6.8143, -8.444, -8.2643, -8.3785, -8.2328, -7.7245, -7.3275, -8.3453, -6.9074, -8.2839, -6.7214, -8.3012, -7.1118, -8.0153, -5.2206, -7.7061, -4.5647, -7.7419, -4.5618, -4.9463, -5.6684, -4.6112, -7.0511, -7.6821, -5.6474, -4.5825, -6.6275, -4.5844, -5.5878, -5.3257, -4.755, -5.3903, -5.2499, -6.3907, -4.1604, -6.1422, -5.5091, -5.6179, -5.0849, -5.8727, -5.1729, -4.69, -5.1062, -4.7399, -5.8893, -5.7767, -5.5347, -5.7608, -4.8109, -5.1546, -4.867, -5.4111, -5.4734, -5.3558, -5.6292, -5.5869, -5.5904, -6.5483, -6.7749, -7.1752, -7.1916, -7.1729, -7.2069, -6.9599, -7.2959, -7.396, -6.1896, -7.059, -6.4424, -6.8035, -6.7374, -7.4416, -7.4748, -7.113, -7.7469, -6.5414, -7.4519, -7.6507, -7.5003, -7.7497, -7.6928, -7.8781, -7.8543, -7.8199, -7.6381, -7.7298, -7.7903, -5.8629, -6.7889, -5.076, -6.4477, -7.046, -5.6265, -5.8903, -6.8151, -6.3209, -5.9565, -6.6934, -4.9792, -6.2957, -5.7217, -6.6124, -5.4918, -6.1235, -5.4571, -6.8579, -6.8574, -6.7574, -6.1685, -5.6965, -6.5857, -4.4301, -5.2915, -5.0089, -4.7235, -4.7078, -4.8638, -6.1561, -5.7495, -5.8149, -5.6227, -5.7919, -5.7355, -5.8568, -5.9219, -5.9701, -5.9101, -5.9309, -6.0849, -6.0785, -6.0742, -6.1169], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9264, 0.919, 0.9173, 0.9165, 0.9067, 0.9064, 0.9063, 0.9057, 0.9045, 0.9044, 0.9009, 0.9003, 0.8985, 0.8977, 0.8954, 0.8944, 0.8937, 0.893, 0.8921, 0.891, 0.8908, 0.8905, 0.8894, 0.8893, 0.8891, 0.8886, 0.8861, 0.8853, 0.8847, 0.8846, 0.8782, 0.8661, 0.8605, 0.8708, 0.858, 0.7857, 0.8047, 0.8506, 0.8204, 0.8684, 0.7553, 0.7839, 0.773, 0.8356, 0.8325, 0.8292, 0.7034, 0.7472, 0.7345, 0.8417, 0.741, 0.785, 0.7757, 0.5517, 0.5416, 0.6245, 0.512, 0.4852, 0.6289, 0.5264, 0.6246, 0.469, 0.262, 0.447, 0.6874, 0.1877, 0.406, 0.4898, 0.5816, 0.3644, 0.4615, 0.167, 0.2238, 0.3525, 0.3416, 0.2775, -0.0411, 0.0199, 0.2437, 0.3728, 0.177, 0.2649, 0.1664, 0.0059, 0.0831, -0.1123, -0.6379, 0.1086, 0.1462, -0.6174, 1.0718, 1.069, 1.0638, 1.0622, 1.0527, 1.0496, 1.0468, 1.0454, 1.0448, 1.0441, 1.0426, 1.0419, 1.0417, 1.0414, 1.0404, 1.0402, 1.0384, 1.0362, 1.0354, 1.0312, 1.0308, 1.0281, 1.0279, 1.0269, 1.0232, 1.023, 1.0226, 1.0221, 1.022, 1.0213, 1.0135, 1.017, 1.0094, 0.9161, 0.9523, 0.9703, 0.8287, 0.8903, 0.8794, 0.93, 0.8373, 0.9411, 0.8661, 0.7483, 0.7256, 0.7576, 0.7019, 0.6754, 0.6804, 0.7397, 0.5456, 0.5619, 0.6791, 0.7342, 0.7447, 0.8148, 0.6043, 0.7715, 0.6173, 0.6053, 0.7438, 0.7457, 0.68, 0.6481, 0.1747, 0.5043, 0.5841, 0.5772, 0.3582, 0.4323, 0.1516, 0.2474, 0.5347, 0.3037, -0.1253, -0.1777, -0.1329, 0.0823, 0.2627, -0.0457, 0.2415, -0.0446, 0.1203, 0.0149, -0.0906, -0.0499, 1.8315, 1.8314, 1.8297, 1.8272, 1.8256, 1.8222, 1.8171, 1.8164, 1.8127, 1.812, 1.8119, 1.8078, 1.8065, 1.8033, 1.8032, 1.8024, 1.8021, 1.8002, 1.8, 1.7973, 1.7964, 1.796, 1.7953, 1.7914, 1.79, 1.7881, 1.7858, 1.7842, 1.7836, 1.7814, 1.7782, 1.7774, 1.7729, 1.705, 1.5975, 1.5761, 1.6164, 1.6918, 1.7012, 1.4246, 1.5204, 1.5231, 1.4141, 1.3378, 1.6245, 1.6611, 1.7129, 1.2398, 1.6219, 1.1742, 1.0771, 1.2008, 1.5679, 1.336, 0.9005, 0.9611, 1.4435, 0.9954, 1.4079, 1.5177, 0.8986, 1.023, 0.9088, 1.2653, 1.2908, 0.7367, 1.2766, 0.1719, 0.8771, 1.1787, 0.5452, 0.3382, 0.3093, 0.5151, 0.0747, 0.1836, -0.0295, -0.2323, -0.2407, 0.3141, 0.2522, -0.0866, -0.6368, 0.158, -0.5003, 0.2314, -0.0678, 0.1365, 0.1394, -0.0632, 2.4164, 2.4143, 2.4056, 2.3963, 2.3882, 2.3873, 2.3803, 2.3709, 2.3627, 2.3612, 2.3573, 2.3572, 2.3514, 2.3513, 2.3505, 2.3504, 2.3476, 2.3456, 2.3443, 2.3426, 2.3422, 2.3416, 2.3403, 2.3319, 2.3294, 2.3281, 2.328, 2.3265, 2.3258, 2.3243, 2.3042, 2.3206, 2.3169, 2.3155, 2.2893, 2.2374, 2.1772, 2.3002, 2.0736, 2.2876, 1.9764, 2.2887, 2.0317, 2.2299, 1.5892, 2.1493, 1.3172, 2.1465, 1.2933, 1.3933, 1.5585, 1.2501, 1.945, 2.124, 1.4812, 1.1209, 1.7904, 1.0512, 1.3951, 1.2543, 0.9632, 1.1969, 1.0461, 1.5629, 0.3659, 1.4032, 1.0584, 1.0977, 0.7585, 1.2225, 0.655, 0.1674, 0.4065, -0.0156, 1.0886, 0.977, 0.7151, 0.9002, -0.2407, 0.0934, -0.518, 0.212, 0.1733, -1.1478, 0.1835, -0.2945, -0.3759, 3.3481, 3.3449, 3.3427, 3.3426, 3.338, 3.337, 3.334, 3.3264, 3.3209, 3.3169, 3.3146, 3.3144, 3.3119, 3.3108, 3.3107, 3.3036, 3.2984, 3.2968, 3.2931, 3.2922, 3.2911, 3.2877, 3.2865, 3.2815, 3.2806, 3.2786, 3.2783, 3.2779, 3.2719, 3.2717, 3.2615, 3.2693, 3.2297, 3.255, 3.2667, 3.2374, 3.2423, 3.2563, 3.2296, 3.2056, 3.2179, 3.0483, 3.1266, 2.9409, 3.1344, 2.6474, 2.8916, 2.3483, 3.132, 3.1182, 3.0426, 2.58, 2.0183, 2.8504, 0.0963, 1.1069, 0.2391, -0.3745, -0.4998, -0.2936, 1.9156, 0.1912, 0.2077, -0.7653, -0.2826, -0.8146, -0.2101, -0.0996, 0.1723, -0.2935, -1.2067, 0.1407, -0.079, -0.9906, 0.7103]}, \"token.table\": {\"Topic\": [1, 4, 2, 1, 2, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 1, 2, 3, 4, 5, 5, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 5, 1, 5, 5, 5, 1, 1, 2, 3, 4, 5, 2, 4, 1, 3, 3, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 2, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 3, 2, 4, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 5, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 2, 3, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 5, 3, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 4, 2, 3, 4, 1, 3, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 1, 4, 1, 1, 2, 3, 4, 3, 1, 3, 1, 2, 3, 4, 3, 1, 2, 3, 4, 1, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 4, 1, 2, 2, 1, 3, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 2, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 2, 3, 1, 4, 1, 2, 3, 4, 5, 1, 3, 4, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 4, 2, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 4, 5, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 5, 2, 2, 1, 2, 3, 4, 5, 1, 2, 2, 5, 2, 4, 5, 5, 4, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 4, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 4, 5, 2, 1, 2, 3, 4, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 3, 1, 2, 3, 4, 5, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 4, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 3, 4, 2, 2, 3, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 5, 1], \"Freq\": [0.0857626735311176, 0.8576267353111761, 0.9787661218664497, 0.09659899822492571, 0.048299499112462854, 0.8693909840243313, 0.1481854749339888, 0.0740927374669944, 0.0740927374669944, 0.7038810059364466, 0.20348850343108066, 0.2576545079415122, 0.19470482702398365, 0.3213361618929655, 0.022691164051667267, 0.9407175421129219, 0.8729015613541743, 0.02352129955744781, 0.028748255014658435, 0.06795042094373811, 0.0052269554572106245, 0.8992167004753343, 0.8930442935572537, 0.04059292243442062, 0.4513702341413275, 0.4280349930852766, 0.08467358897481328, 0.030669173959381187, 0.005333769384240207, 0.15187900553656436, 0.3081603010886813, 0.15187900553656436, 0.3477809112286546, 0.03962061013997331, 0.9223935923104657, 0.05950926402003004, 0.9343294828903137, 0.07363869354252885, 0.03681934677126442, 0.1472773870850577, 0.03681934677126442, 0.6995675886540241, 0.8340143862273373, 0.8995385560709325, 0.2000864602358166, 0.3648635451359009, 0.11328424586880793, 0.30160091432604713, 0.020597135612510534, 0.9154747509365315, 0.165304831314116, 0.694102544388842, 0.06398896696030297, 0.0559903460902651, 0.019552184348981462, 0.9006707051835469, 0.09312074142517994, 0.018624148285035988, 0.018624148285035988, 0.8567108211116554, 0.018624148285035988, 0.1628018885325008, 0.7597421464850037, 0.05039106073625024, 0.019381177206250093, 0.0077524708825000375, 0.21080623490127232, 0.7042489773368431, 0.051530412975866566, 0.012492221327482804, 0.020299859657159556, 0.034932018271162574, 0.9431644933213896, 0.10120181934747857, 0.08096145547798286, 0.7893741909103329, 0.020240363869495716, 0.020240363869495716, 0.9105201415207038, 0.06147701427880668, 0.8606781999032935, 0.9440439838379147, 0.9058121949622524, 0.9893258241824473, 0.1800564623708603, 0.1820351048144961, 0.5342334597816734, 0.08508162507634057, 0.019786424436358273, 0.9716847339291889, 0.8995218206653162, 0.06491858519785171, 0.9088601927699239, 0.9572761552407228, 0.05871158636995588, 0.7168275080052753, 0.05871158636995588, 0.12151932992851333, 0.04369234334508344, 0.9201412637406828, 0.6980725813062034, 0.2103780382018695, 0.05163824574045888, 0.036338024780322914, 0.005737582860050987, 0.7278747177093323, 0.1216561142913615, 0.06804494528160897, 0.07216888135928225, 0.010309840194183177, 0.9876438563369612, 0.10525271542532749, 0.40270604162734, 0.15101476561025248, 0.0961003053883425, 0.24711507099859498, 0.8301510039343039, 0.09237770570096013, 0.04244381072746817, 0.021221905363734084, 0.01373182111771029, 0.8550083713317128, 0.030903917036086002, 0.0868252907204321, 0.01618776606652124, 0.010301305678695334, 0.9437688589265589, 0.013677809549660275, 0.041033428648980824, 0.06534414075114406, 0.9148179705160169, 0.9319910934440558, 0.10112590603920746, 0.37702375838530605, 0.4539673825455726, 0.028579060402384716, 0.03957100671099422, 0.0709494036525659, 0.21284821095769768, 0.7094940365256589, 0.05601591261540148, 0.7700500758334108, 0.1208054019055044, 0.03374452567192861, 0.019571824889718592, 0.9246047621752839, 0.9384609355657396, 0.07081843291088126, 0.09335157065525257, 0.6373658961979314, 0.09978961001078723, 0.0965705903330199, 0.031045288210391556, 0.9624039345221382, 0.3657403495317339, 0.06513184306729508, 0.03006085064644388, 0.1903853874274779, 0.34569978243410465, 0.39679506273670007, 0.21517166667354581, 0.03470510752799126, 0.3470510752799126, 0.006941021505598252, 0.9536770902717044, 0.06221994276792143, 0.8088592559829786, 0.07999706927304183, 0.02222140813140051, 0.026665689757680612, 0.34822828227434677, 0.057010824974118725, 0.41910660521514304, 0.13559331345195805, 0.040061660792623965, 0.9639331059697603, 0.02605224610729082, 0.3486993802179115, 0.36493683680547434, 0.21068099922362757, 0.06413795352087313, 0.011772156025983043, 0.3129225646106668, 0.2047518015353746, 0.12362372922890541, 0.35155497999469976, 0.007726483076806588, 0.8815385182781327, 0.8552487781018808, 0.0782085903279006, 0.01766000426759046, 0.04541143954523261, 0.005045715505025846, 0.9521171454904681, 0.04028192425149092, 0.9667661820357821, 0.9734393549771595, 0.03743997519142921, 0.09703305040080258, 0.0920991664821177, 0.7483057276672064, 0.027958675539214304, 0.036181815403689095, 0.07055290520906426, 0.3480609990313837, 0.5032773904913251, 0.056442324167251416, 0.018814108055750473, 0.3285450510438396, 0.04599630714613755, 0.5650974877954041, 0.006570901020876792, 0.05913810918789113, 0.24243920852372178, 0.14325953230947197, 0.5895680752735961, 0.016529946035708305, 0.011019964023805536, 0.0566717016690558, 0.0755622688920744, 0.850075525035837, 0.9439295281410558, 0.6703630263354027, 0.1965925916069477, 0.06641641608342827, 0.05490423729563404, 0.011512178787794234, 0.2934560197426131, 0.3022159009289597, 0.13358818809178655, 0.2562265247006398, 0.015329792076106652, 0.3999448520640213, 0.3668205851752834, 0.09446550186788233, 0.03803156568706951, 0.10059962536579677, 0.9531518421681034, 0.9428337356488117, 0.5547074303846022, 0.2817964520489284, 0.07933458672548659, 0.062198315992781486, 0.02157900758933235, 0.024786507836886282, 0.049573015673772564, 0.06609735423169676, 0.024786507836886282, 0.8344790971751715, 0.9083409666868907, 0.012793534742068883, 0.0767612084524133, 0.2648052078949934, 0.35917727666608146, 0.17747583082324025, 0.1838142533526417, 0.014789652568603356, 0.10945069409479442, 0.3702693693845173, 0.38036056813084584, 0.10013574140587575, 0.040364794985314256, 0.929557550738971, 0.954905868606517, 0.035366884022463595, 0.038113352889570344, 0.038113352889570344, 0.8766071164601179, 0.12530009713088114, 0.4285626510563471, 0.37226840451928456, 0.05084641622702423, 0.02360726467683268, 0.43012549367608566, 0.43727259402417773, 0.11110492359306745, 0.012994727905621924, 0.009096309533935346, 0.11605086558687416, 0.7065449757789103, 0.07850499730876781, 0.08191825806132294, 0.01706630376277561, 0.22635356245223445, 0.5980205230219527, 0.1397244212668114, 0.02515039582802605, 0.009780709488676797, 0.9572271930881247, 0.0961065726318947, 0.5194074318645096, 0.32287489007793835, 0.04427381435851329, 0.017277586091127138, 0.8568944281966797, 0.061206744871191404, 0.020402248290397135, 0.061206744871191404, 0.822099171410026, 0.09854816565798419, 0.03630721892662576, 0.01815360946331288, 0.020746982243786145, 0.5854666141852934, 0.3198433140170359, 0.05695839838659544, 0.024097783932790377, 0.013691922689085442, 0.8345865610469951, 0.13991421821557112, 0.05596568728622845, 0.04896997637544989, 0.27982843643114225, 0.46871263102216326, 0.9730960909248596, 0.16725811820625366, 0.6601575522671318, 0.12971037738444163, 0.030038192657449638, 0.012971037738444163, 0.9704045476314934, 0.22885043671341, 0.26154335624389713, 0.23538902061950742, 0.045770087342681995, 0.22231185280731255, 0.2273555780750296, 0.4553409499951424, 0.21161073194795, 0.08376258139606355, 0.022042784577911458, 0.9669781243582739, 0.9557833811534268, 0.2469817315212049, 0.15093328037406967, 0.5145452740025102, 0.05259796134247882, 0.03430301826683401, 0.054910397399903624, 0.027455198699951812, 0.027455198699951812, 0.9060215570984098, 0.8653302093428855, 0.12915376258849037, 0.012915376258849037, 0.8855127422202177, 0.033405097352615744, 0.033405097352615744, 0.033405097352615744, 0.06681019470523149, 0.8685325311680094, 0.9332846634470773, 0.01904662578463423, 0.01904662578463423, 0.01904662578463423, 0.01904662578463423, 0.03747393608537794, 0.01873696804268897, 0.01873696804268897, 0.05621090412806691, 0.8619005299636926, 0.25636246250718364, 0.24154382305590139, 0.19412417681179803, 0.28451787746462004, 0.02222795917692344, 0.358548494093939, 0.3356624200028365, 0.12413961400931352, 0.16575065781131804, 0.015950900124101736, 0.016232822522375435, 0.03246564504475087, 0.25972516035800697, 0.6817785459397683, 0.023556587141529518, 0.9422634856611807, 0.023556587141529518, 0.023556587141529518, 0.043886426897480076, 0.1755457075899203, 0.043886426897480076, 0.7460692572571613, 0.9441964400231767, 0.20101630315243996, 0.26961710502192343, 0.2775939424486076, 0.21218387554979773, 0.04147955461875745, 0.5490785389443922, 0.19623425446809892, 0.1811393118167067, 0.05786394683033686, 0.015723898595200233, 0.8130941743572897, 0.07912661428309195, 0.04365606305274039, 0.04911307093433294, 0.013642519703981372, 0.12499320655480459, 0.3228991169332452, 0.3801876699375306, 0.13150326939620066, 0.04036238961665565, 0.08112172814768737, 0.8112172814768737, 0.8872683304549888, 0.2060300571243758, 0.5800386871386733, 0.121449296831211, 0.0678223345940529, 0.02464474367516457, 0.9563838560117794, 0.16390497422855935, 0.444129607587064, 0.19245616328772774, 0.17659439158818974, 0.023263931825989067, 0.31248352028250637, 0.38751755706766744, 0.17571261778803549, 0.08453201612505491, 0.038941715293564626, 0.24941425283779442, 0.29502143049956253, 0.12256928996600182, 0.31426195857562095, 0.0185279159250933, 0.8845766608826454, 0.08903875563805547, 0.04451937781902773, 0.04451937781902773, 0.7568294229234714, 0.911747300324577, 0.06953642994666642, 0.6625276519918496, 0.21923291108185108, 0.03573399872259247, 0.012555188740370328, 0.8791313432272895, 0.9343710543929721, 0.39243924899690974, 0.3214621982213921, 0.2166865518384851, 0.05933531228852798, 0.010139578682216807, 0.8861757114065756, 0.48788774318402073, 0.3211955095235788, 0.14165545548219372, 0.03689630468373418, 0.012188957797305042, 0.3269668029785692, 0.19409827068203453, 0.19899664975287454, 0.26451246982535936, 0.015307434596374963, 0.11781420442897551, 0.7068852265738531, 0.11434908076929977, 0.041581483916109004, 0.013860494638703001, 0.04719093888068533, 0.8966278387330213, 0.890032890235702, 0.9469266047935295, 0.023673165119838236, 0.015782110079892157, 0.007891055039946079, 0.007891055039946079, 0.900139186706117, 0.4685084796868174, 0.10694215297199093, 0.04328610953628204, 0.37939001887682494, 0.0025462417374283552, 0.2690429617396915, 0.09783380426897872, 0.01222922553362234, 0.6236905022147394, 0.9482079094445904, 0.9318376283663526, 0.04659188141831763, 0.9144669073208164, 0.9515404810454522, 0.8982672973779277, 0.931671724863062, 0.3521702104727344, 0.2758256193912325, 0.06895640484780813, 0.29306472060318456, 0.009850914978258305, 0.2647144993628921, 0.14994317796079903, 0.1591989296867743, 0.4220622787044713, 0.0055534510355851495, 0.0266531395450574, 0.9595130236220665, 0.06976628222405547, 0.22674041722818028, 0.6278965400164992, 0.0523247116680416, 0.0261623558340208, 0.6604820921548348, 0.22501005195289525, 0.06692109303771454, 0.0397647074571927, 0.008728838222310593, 0.9478680012580161, 0.9122792988625493, 0.13388980269242154, 0.047144296722683644, 0.7637376069074749, 0.0207434905579808, 0.03205812177142488, 0.9530614486929098, 0.9584273681034059, 0.07391324082550398, 0.8869588899060478, 0.9517821417525756, 0.15277297180828212, 0.13367635033224684, 0.05728986442810579, 0.649285130185199, 0.9278795708082371, 0.9479531394610942, 0.033855469266467646, 0.1167562695398679, 0.0778375130265786, 0.0778375130265786, 0.7394563737524967, 0.9671827627428502, 0.9230289265754152, 0.0466176225543139, 0.01864704902172556, 0.00932352451086278, 0.9343594605249026, 0.10094524277523988, 0.0757089320814299, 0.05047262138761994, 0.02523631069380997, 0.757089320814299, 0.922614517895458, 0.057466923376716224, 0.057466923376716224, 0.11493384675343245, 0.8045369272740271, 0.9050237292070498, 0.9365818962137202, 0.9726094794696474, 0.027788842270561352, 0.9653790210534559, 0.0414943866858141, 0.9543708937737243, 0.9709946116949227, 0.5613217507273376, 0.3091163536556096, 0.08027440191831721, 0.03893907555739268, 0.010184065915010393, 0.424153159608252, 0.10458571058833611, 0.05229285529416806, 0.4125325250984369, 0.005810317254907562, 0.049415905423078846, 0.938902203038498, 0.011807236701780703, 0.3069881542462983, 0.035421710105342105, 0.011807236701780703, 0.6257835451943773, 0.06882076720518013, 0.15641083455722757, 0.03753860029373461, 0.03753860029373461, 0.7007205388163794, 0.06854140604968585, 0.925308981670759, 0.05775599616814941, 0.043316997126112054, 0.014438999042037352, 0.043316997126112054, 0.8374619444381665, 0.03526150478519777, 0.03526150478519777, 0.03526150478519777, 0.916799124415142, 0.049929095561358876, 0.9236882678851392, 0.9122057837604542, 0.281343228736438, 0.33926683465276347, 0.26686232725735665, 0.08274800845189353, 0.02965136969526185, 0.9147060834707864, 0.46550708132934776, 0.30713347648577616, 0.08905352860213608, 0.12953240523947068, 0.009107747243400282, 0.5147885493621914, 0.2913579685048826, 0.10666284010776048, 0.06287493732667987, 0.024700868235481375, 0.9217083317840123, 0.03870817947738778, 0.8748048561889638, 0.046449815372865334, 0.023224907686432667, 0.023224907686432667, 0.9569951245608245, 0.06093266637804014, 0.06093266637804014, 0.853057329292562, 0.03046633318902007, 0.9581916854658921, 0.03422113162378186, 0.0508038121685051, 0.07620571825275765, 0.8128609946960816, 0.02540190608425255, 0.0508038121685051, 0.03900673097019452, 0.09751682742548629, 0.8386447158591821, 0.01950336548509726, 0.01950336548509726, 0.8751699730218395, 0.1529661892872667, 0.6150964372044316, 0.15942954939799628, 0.05386133425607983, 0.01831285364706714, 0.0848087485219194, 0.6022858581471904, 0.2458016270720037, 0.03449847397501806, 0.031623601143766555, 0.9250402358017111, 0.017453589354749267, 0.03490717870949853, 0.10332050205499889, 0.6634652239103143, 0.1734308427351767, 0.03911419006367815, 0.020664100410999777, 0.06080384985746297, 0.9120577478619445, 0.981680879903236, 0.861122980157464, 0.08882369215552934, 0.14063751257958812, 0.7105895372442347, 0.03700987173147056, 0.029607897385176444, 0.9440669203754771, 0.028608088496226577, 0.028608088496226577, 0.058811110235162764, 0.07841481364688369, 0.7841481364688369, 0.03920740682344184, 0.03920740682344184, 0.9207554246666614, 0.7322593355318161, 0.17996204008832767, 0.03723352553551607, 0.033096467142680955, 0.020685291964175595, 0.9248868208652902, 0.967409812314181, 0.9380807460476068, 0.65093909917684, 0.1772646654008359, 0.07119646397246689, 0.09444428894306832, 0.006538450772981652, 0.5661423505017672, 0.2368464887791219, 0.07131808769928949, 0.11270018796924759, 0.013207053277646203, 0.33583487337984347, 0.25230524558860473, 0.14531866752722358, 0.24658403820564315, 0.020024225840365454, 0.2538555018896725, 0.6230998682746506, 0.07015642961314585, 0.03415510389061048, 0.019385329235211354, 0.028598283662248623, 0.028598283662248623, 0.028598283662248623, 0.9437433608542046, 0.1129867924159312, 0.19238291681631528, 0.6412763893877176, 0.04580545638483697, 0.003053697092322465, 0.1298695535450861, 0.8225071724522119, 0.04328985118169537, 0.8518292725422048, 0.8194661246501319, 0.05237189518440693, 0.024645597733838554, 0.0955016912186244, 0.009242099150189458, 0.8734548301860448, 0.21027318514767113, 0.5893808984498637, 0.16659460074948643, 0.01875939201716906, 0.014559528132728226, 0.9542269035205153, 0.9618333427156478, 0.9654911779361738, 0.03791892268811812, 0.15167569075247248, 0.7773379151064214, 0.01895946134405906, 0.012639640896039372, 0.9557199220818793, 0.03413285436006712, 0.05047234586068987, 0.9085022254924175, 0.2011150407429375, 0.7240141466745751, 0.0402230081485875, 0.8378035359671062, 0.8360008673912508, 0.4270517919671635, 0.006373907342793485, 0.5545299388230333, 0.006373907342793485, 0.006373907342793485, 0.052523699477454576, 0.8929028911167277, 0.6233889934509702, 0.25167454987243765, 0.0682044850602812, 0.042968825587977155, 0.01364089701205624, 0.9723380139356897, 0.20646025913393531, 0.2810635460478783, 0.3834261955344513, 0.09889272916499423, 0.029494322733419332, 0.916531711486491, 0.020830266170147525, 0.05207566542536881, 0.010415133085073762, 0.010415133085073762, 0.8447588171143217, 0.10493898349246232, 0.03410516963505025, 0.007870423761934675, 0.007870423761934675, 0.9852759772923063, 0.904931732637599, 0.37438300111724127, 0.28135335416623325, 0.18360617947937286, 0.12416533000763344, 0.03641931412078002, 0.10891187625415069, 0.6627403533763212, 0.17147827325121598, 0.03475910944281405, 0.0231727396285427, 0.8980477308600454, 0.020884830950233614, 0.06265449285070084, 0.010442415475116807, 0.8988551340389002, 0.06355781099216351, 0.4550739267038907, 0.4296508023070253, 0.040676999034984646, 0.012711562198432702, 0.053113583061718185, 0.643264505969698, 0.2517977271074047, 0.041310564603558586, 0.011803018458159596, 0.9297524754117801, 0.1543869074137, 0.3087738148274, 0.06432787808904167, 0.02573115123561667, 0.43742957100548335, 0.03296046443575996, 0.09888139330727988, 0.26368371548607966, 0.6097685920615592, 0.9208623644863025, 0.4978315538946176, 0.25767444787281, 0.1260118462185014, 0.10623420219317606, 0.011866586415195198, 0.8994113406744461, 0.09315174158362889, 0.8487158677619521, 0.020700387018584197, 0.031050580527876296, 0.418607136194255, 0.07023609667688842, 0.09271164761349272, 0.41017880459302836, 0.008428331601226611, 0.9796099297157594, 0.0709309973127854, 0.0709309973127854, 0.8511719677534249, 0.7743459716056199, 0.13077843076006027, 0.041298451818966395, 0.03441537651580533, 0.017207688257902667, 0.04115710014969886, 0.08231420029939772, 0.04115710014969886, 0.8231420029939772, 0.9668458765581253, 0.9258791531096688, 0.018334240655637005, 0.027501360983455507, 0.027501360983455507, 0.9267251758326202, 0.32266289330548603, 0.23446836913531982, 0.1340843578847242, 0.300435005099997, 0.007887315169689659, 0.2126182856198488, 0.07395418630255611, 0.16639691918075122, 0.5176793041178928, 0.036977093151278055, 0.9720522030968689, 0.9606368955666745, 0.9009338163349574, 0.029701114824229363, 0.009900371608076455, 0.04950185804038227, 0.009900371608076455, 0.8862647515095143, 0.039609597832827456, 0.009902399458206864, 0.059414396749241184, 0.15563105185639356, 0.6150030041760087, 0.10008985862883303, 0.10240407501331472, 0.02661348842153942, 0.6365932513991208, 0.27313291529624445, 0.04516371040331601, 0.026883160954354765, 0.01720522301078705, 0.09460065237727265, 0.8198723206030296, 0.04504792970346317, 0.022523964851731585, 0.018019171881385267, 0.05594530695011816, 0.7002186805369627, 0.19671091798589932, 0.04331249570331728, 0.003609374641943107, 0.8939008745873489, 0.12076298252242489, 0.3342859371272921, 0.30978330299230733, 0.2292746479773574, 0.005250564457496734, 0.20470882184422873, 0.05117720546105718, 0.15353161638317156, 0.562949260071629, 0.31898677694978717, 0.556714049744344, 0.020747107443888597, 0.09249752068733666, 0.010373553721944298, 0.5068129635338923, 0.29647057049877823, 0.12263388204000636, 0.05136083977190988, 0.022756802852784686, 0.11766818933103639, 0.5731579544834353, 0.23786687735736387, 0.04301847781994878, 0.027835485648202155, 0.09660541749193646, 0.8694487574274281, 0.048007932052273265, 0.25924283308227564, 0.6529078759109165, 0.028804759231363962, 0.009601586410454653, 0.11434883714724937, 0.4251697672111363, 0.40437906954800007, 0.02598837207892031, 0.03014651161154756, 0.6098424237834058, 0.2709685287688581, 0.10120511315463374, 0.010446979422413804, 0.007835234566810354, 0.8495746134017421, 0.044380763386658174, 0.019020327165710647, 0.07608130866284259, 0.012680218110473764, 0.04382109882561244, 0.04382109882561244, 0.9202430753378612, 0.10523277784151172, 0.7106041342416061, 0.1301265532448801, 0.03960373359626786, 0.014709958192899488, 0.3977974968867813, 0.3912288460134857, 0.0938003344706611, 0.10194546155354764, 0.015502016060977605, 0.0860648900159809, 0.028688296671993633, 0.028688296671993633, 0.8319606034878154, 0.7289592671023104, 0.03865693083118313, 0.1214932111837184, 0.10124434265309867, 0.011044837380338037, 0.07197372212114288, 0.7277343014471113, 0.15727591130175664, 0.029322627530835983, 0.013328467059470902, 0.8572175529398511, 0.9407247313196401, 0.12500022504093192, 0.06944456946718439, 0.7083346085652807, 0.09027794030733971, 0.006944456946718439, 0.931776651018613, 0.09001718850712635, 0.9001718850712634, 0.8822291151845665, 0.9159122091733048, 0.10543082914100699, 0.31265694159057245, 0.49988755196167106, 0.07089314373274608, 0.009088864581121292, 0.9157295920694221, 0.9432272977776377, 0.434990407579975, 0.2603728296800136, 0.06835563547685321, 0.2255735970736156, 0.01118546762348507, 0.9257320739018869, 0.7882044400832913, 0.13095559662825842, 0.03582747454924051, 0.03706290470611087, 0.007412580941222174, 0.9653268988014867, 0.31090350088645735, 0.22809275532324208, 0.20339481366403753, 0.24843223668964584, 0.008716920585601608, 0.9210439705718217, 0.8665905358995948, 0.47048437299227563, 0.40021366150684823, 0.08179950008850534, 0.027312249190625103, 0.020038132572016407, 0.19029430275833353, 0.6777605303721468, 0.10253300331270938, 0.010427085082648412, 0.019116322651522088, 0.020967633853659174, 0.8406115026785177, 0.09721357513969253, 0.03621682211086585, 0.005718445596452502, 0.9120729960228854, 0.5083292330137554, 0.3114459808208434, 0.10907469372359935, 0.062426397036776986, 0.00891805671953957, 0.9327405051992481, 0.05481001871506524, 0.02740500935753262, 0.1370250467876631, 0.7673402620109134, 0.9649267080761627, 0.9581636474131929, 0.11937246776431923, 0.5178295325776331, 0.25850313708962924, 0.07326999745534077, 0.030460560739860768, 0.0847700470034092, 0.4634095902853037, 0.2467750257210357, 0.1375158540277527, 0.06781603760272736, 0.19280959161675548, 0.19538038617164555, 0.5887119530698267, 0.010283178219560292, 0.012853972774450365, 0.9184911401247212, 0.020186618464279587, 0.05046654616069897, 0.010093309232139793, 0.6333214567820424, 0.24941113040282498, 0.07965486363650431, 0.03264543591660013, 0.004570361028324018, 0.8796347772054283, 0.0318785830460549, 0.1593929152302745, 0.7491467015822901, 0.047817874569082354, 0.4346557278534537, 0.24091383365520883, 0.05728021219774196, 0.2577609548898388, 0.008423560617314994, 0.06817824245478121, 0.8863171519121558, 0.027271296981912486, 0.45220376332105, 0.3173598700415803, 0.17706773864980874, 0.02996530961765994, 0.023836041741320407, 0.07580458915035763, 0.07580458915035763, 0.833850480653934, 0.9322820243756166, 0.9692917543503695, 0.03230972514501232, 0.942431353878137, 0.4609133727623677, 0.29416577034085245, 0.15018784052586134, 0.0848687797152264, 0.00988985779879332, 0.951658188431379, 0.00704931991430651, 0.021147959742919532, 0.00704931991430651, 0.00704931991430651, 0.2647130552252333, 0.10829170441032271, 0.547474727852187, 0.054145852205161354, 0.024064823202293934, 0.057120748276361204, 0.9139319724217793, 0.04766399887062121, 0.04766399887062121, 0.04766399887062121, 0.8579519796711819, 0.24696446934066174, 0.43024627677586985, 0.18561166091955394, 0.0947473750300652, 0.04193736271822558, 0.09727974998043419, 0.058367849988260515, 0.019455949996086838, 0.019455949996086838, 0.8171498998356472, 0.9325614095055419, 0.9833650432867023, 0.0796443823391124, 0.0796443823391124, 0.8760882057302364, 0.04233236571728839, 0.04233236571728839, 0.8466473143457678, 0.04233236571728839, 0.04233236571728839, 0.42359633972876554, 0.34089419721029224, 0.14422446805050826, 0.06555657638659466, 0.025214067840997948, 0.3034321954308714, 0.3853810365326396, 0.059800505668857876, 0.24473910653365907, 0.0066445006298730975, 0.9621664570915781, 0.012179322241665546, 0.024358644483331093, 0.9569759738229101, 0.9632032534456653, 0.38351188566642996, 0.4805520143123297, 0.09181042111408476, 0.038351188566643, 0.00581078614646106, 0.07383595966351342, 0.7752775764668908, 0.08519533807328471, 0.05395704744641365, 0.00851953380732847, 0.9502901495652031, 0.016106612704494965, 0.016106612704494965, 0.016106612704494965, 0.1497798093325697, 0.7084180171135054, 0.1113228312606937, 0.0242886177296059, 0.008096205909868632, 0.4605852932603336, 0.3791743334346572, 0.08754466227829585, 0.06356746178169254, 0.008921749021991934, 0.9411551694231189, 0.013840517197398807, 0.013840517197398807, 0.027681034394797615, 0.8909248042014075, 0.0375126233347961, 0.009378155833699026, 0.056268935002194154, 0.009378155833699026, 0.3091910564230779, 0.41618632662942917, 0.14490698012198747, 0.08930313891238763, 0.04043915724334534, 0.06333827834594412, 0.8007768048022935, 0.1040557429969082, 0.03166913917297206, 0.0045241627389960085, 0.6768255983957773, 0.14479013810283026, 0.0641994008569153, 0.10381179713033112, 0.00956161289358313, 0.8771912840470967, 0.859633367739785, 0.2015632537272028, 0.1007816268636014, 0.08398468905300116, 0.016796937810600234, 0.6046897611816084, 0.6182212440799701, 0.175455111007671, 0.07224622217962923, 0.09598426661007883, 0.03818728886637545, 0.06982334643564762, 0.13964669287129525, 0.7680568107921238, 0.9858507984632191], \"Term\": [\"acai\", \"acai\", \"adequate\", \"ai\", \"ai\", \"ai\", \"amazed\", \"amazed\", \"amazed\", \"amazed\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"appliance\", \"appointment\", \"appointment\", \"appointment\", \"appointment\", \"appointment\", \"asado\", \"asia\", \"asia\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"atmosphere\", \"atmosphere\", \"atmosphere\", \"atmosphere\", \"atmosphere\", \"attach\", \"attach\", \"attorney\", \"au\", \"au\", \"au\", \"au\", \"au\", \"aussi\", \"avec\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"baba\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"barbeque\", \"barber\", \"barber\", \"barber\", \"barber\", \"barber\", \"bartender\", \"bartender\", \"bartender\", \"bartender\", \"bartender\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"bento\", \"bento\", \"blueberry\", \"blueberry\", \"blueberry\", \"blueberry\", \"blueberry\", \"bobbie\", \"bon\", \"bon\", \"bonne\", \"bouchon\", \"brake\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"brewery\", \"broad\", \"brulee\", \"brulee\", \"bumper\", \"burger\", \"burger\", \"burger\", \"burger\", \"burger\", \"burro\", \"business\", \"business\", \"business\", \"business\", \"business\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"cajun\", \"cake\", \"cake\", \"cake\", \"cake\", \"cake\", \"call\", \"call\", \"call\", \"call\", \"call\", \"car\", \"car\", \"car\", \"car\", \"car\", \"cat\", \"cat\", \"cat\", \"ce\", \"ce\", \"cha\", \"cheese\", \"cheese\", \"cheese\", \"cheese\", \"cheese\", \"cheesecake\", \"cheesecake\", \"cheesecake\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chilli\", \"chive\", \"chocolate\", \"chocolate\", \"chocolate\", \"chocolate\", \"chocolate\", \"cirque\", \"cirque\", \"class\", \"class\", \"class\", \"class\", \"class\", \"clean\", \"clean\", \"clean\", \"clean\", \"clean\", \"cobbler\", \"cocktail\", \"cocktail\", \"cocktail\", \"cocktail\", \"cocktail\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"collection\", \"collection\", \"come\", \"come\", \"come\", \"come\", \"come\", \"comfortable\", \"comfortable\", \"comfortable\", \"comfortable\", \"comfortable\", \"comme\", \"company\", \"company\", \"company\", \"company\", \"company\", \"concoction\", \"contractor\", \"contractor\", \"cox\", \"cox\", \"cream\", \"cream\", \"cream\", \"cream\", \"cream\", \"crispy\", \"crispy\", \"crispy\", \"crispy\", \"crispy\", \"crust\", \"crust\", \"crust\", \"crust\", \"crust\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cupcake\", \"cupcake\", \"cupcake\", \"curly\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"d\", \"d\", \"d\", \"d\", \"d\", \"daddy\", \"dash\", \"day\", \"day\", \"day\", \"day\", \"day\", \"de\", \"de\", \"de\", \"de\", \"de\", \"dealership\", \"dealership\", \"dealership\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"dent\", \"depot\", \"depot\", \"des\", \"des\", \"des\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"didn\", \"didn\", \"didn\", \"didn\", \"didn\", \"dining\", \"dining\", \"dining\", \"dining\", \"dining\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"disgusted\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"don\", \"don\", \"don\", \"don\", \"don\", \"donna\", \"donut\", \"donut\", \"donut\", \"donut\", \"donut\", \"doughnut\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"dunkin\", \"e\", \"e\", \"e\", \"e\", \"e\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"eating\", \"efficiency\", \"egg\", \"egg\", \"egg\", \"egg\", \"egg\", \"en\", \"en\", \"en\", \"en\", \"enchilada\", \"enchilada\", \"enchilada\", \"enterprise\", \"est\", \"est\", \"est\", \"est\", \"est\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"et\", \"et\", \"et\", \"et\", \"et\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"fajita\", \"fajita\", \"fajita\", \"fajita\", \"farmer\", \"farmer\", \"farmer\", \"farmer\", \"fav\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"favorite\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fix\", \"fix\", \"fix\", \"fix\", \"fix\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"floral\", \"floral\", \"fluke\", \"food\", \"food\", \"food\", \"food\", \"food\", \"freezer\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"frill\", \"frost\", \"frost\", \"frost\", \"frost\", \"froyo\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"ganoush\", \"gay\", \"get\", \"get\", \"get\", \"get\", \"get\", \"giada\", \"go\", \"go\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"good\", \"grill\", \"grill\", \"grill\", \"grill\", \"grill\", \"grub\", \"grub\", \"guys\", \"gym\", \"gym\", \"gym\", \"gym\", \"gym\", \"hahaha\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"haircut\", \"haircut\", \"haircut\", \"haircut\", \"hakka\", \"halal\", \"halal\", \"halo\", \"harness\", \"hawaii\", \"heartbeat\", \"helpful\", \"helpful\", \"helpful\", \"helpful\", \"helpful\", \"highly\", \"highly\", \"highly\", \"highly\", \"highly\", \"holy\", \"holy\", \"homemade\", \"homemade\", \"homemade\", \"homemade\", \"homemade\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"howard\", \"humor\", \"ice\", \"ice\", \"ice\", \"ice\", \"ice\", \"id\", \"ikea\", \"implant\", \"implant\", \"in\", \"informative\", \"informative\", \"informative\", \"informative\", \"ink\", \"inspect\", \"inspect\", \"installation\", \"installation\", \"installation\", \"installation\", \"installer\", \"insurance\", \"insurance\", \"insurance\", \"insurance\", \"iphone\", \"j\", \"j\", \"j\", \"j\", \"j\", \"jamba\", \"japan\", \"japan\", \"japan\", \"japan\", \"je\", \"jeff\", \"jewelry\", \"jewelry\", \"keg\", \"knife\", \"knife\", \"knot\", \"know\", \"know\", \"know\", \"know\", \"know\", \"knowledgeable\", \"knowledgeable\", \"knowledgeable\", \"knowledgeable\", \"knowledgeable\", \"kobe\", \"kobe\", \"l\", \"l\", \"l\", \"l\", \"l\", \"la\", \"la\", \"la\", \"la\", \"la\", \"layout\", \"layout\", \"le\", \"le\", \"le\", \"le\", \"le\", \"les\", \"les\", \"les\", \"les\", \"library\", \"library\", \"lion\", \"little\", \"little\", \"little\", \"little\", \"little\", \"london\", \"look\", \"look\", \"look\", \"look\", \"look\", \"m\", \"m\", \"m\", \"m\", \"m\", \"mais\", \"margarita\", \"margarita\", \"margarita\", \"margarita\", \"margarita\", \"marry\", \"marshmallow\", \"marshmallow\", \"marshmallow\", \"marshmallow\", \"mashed\", \"mashed\", \"matcha\", \"matcha\", \"matcha\", \"matcha\", \"matcha\", \"mayo\", \"mayo\", \"mayo\", \"mayo\", \"mayo\", \"mayonnaise\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"meat\", \"meat\", \"meat\", \"meat\", \"meat\", \"mechanic\", \"mechanic\", \"mechanic\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"mercedes\", \"mercedes\", \"michael\", \"mikey\", \"milk\", \"milk\", \"milk\", \"milk\", \"milk\", \"minor\", \"minor\", \"minor\", \"mocha\", \"mocha\", \"mocha\", \"mocha\", \"mocha\", \"mojito\", \"money\", \"money\", \"money\", \"money\", \"money\", \"musician\", \"nacho\", \"nancy\", \"need\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"night\", \"night\", \"night\", \"night\", \"night\", \"nigiri\", \"nigiri\", \"nigiri\", \"nigiri\", \"noodle\", \"noodle\", \"noodle\", \"noodle\", \"noodle\", \"noodles\", \"noodles\", \"noodles\", \"nous\", \"office\", \"office\", \"office\", \"office\", \"office\", \"optometrist\", \"order\", \"order\", \"order\", \"order\", \"order\", \"ou\", \"owe\", \"pakora\", \"pancake\", \"pancake\", \"pancake\", \"pancake\", \"pancake\", \"pant\", \"pant\", \"pas\", \"pas\", \"pastor\", \"pastor\", \"pastor\", \"pe\", \"pearson\", \"pedicure\", \"pedicure\", \"pedicure\", \"pedicure\", \"pedicure\", \"penne\", \"penne\", \"people\", \"people\", \"people\", \"people\", \"people\", \"peoria\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"pet\", \"pet\", \"pet\", \"pet\", \"pet\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"physician\", \"pinball\", \"place\", \"place\", \"place\", \"place\", \"place\", \"plate\", \"plate\", \"plate\", \"plate\", \"plate\", \"polish\", \"polish\", \"polish\", \"polish\", \"pond\", \"pork\", \"pork\", \"pork\", \"pork\", \"pork\", \"portion\", \"portion\", \"portion\", \"portion\", \"portion\", \"possibility\", \"pour\", \"pour\", \"pour\", \"pour\", \"pour\", \"poutine\", \"poutine\", \"poutine\", \"poutine\", \"precise\", \"price\", \"price\", \"price\", \"price\", \"price\", \"prickly\", \"prime\", \"prime\", \"prime\", \"prime\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"promotion\", \"prosciutto\", \"prosciutto\", \"prosciutto\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"que\", \"que\", \"que\", \"que\", \"quinoa\", \"quote\", \"quote\", \"quote\", \"quote\", \"ragu\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"relax\", \"relax\", \"relax\", \"relax\", \"relax\", \"removal\", \"renovate\", \"rent\", \"rent\", \"rent\", \"rent\", \"rent\", \"repair\", \"repair\", \"repair\", \"repair\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"review\", \"review\", \"review\", \"review\", \"review\", \"rib\", \"rib\", \"rib\", \"rib\", \"rib\", \"rice\", \"rice\", \"rice\", \"rice\", \"rice\", \"rick\", \"roll\", \"roll\", \"roll\", \"roll\", \"roll\", \"roof\", \"roof\", \"roof\", \"roof\", \"room\", \"room\", \"room\", \"room\", \"room\", \"s\", \"s\", \"s\", \"s\", \"s\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"sammy\", \"sammy\", \"sashimi\", \"sashimi\", \"sashimi\", \"sashimi\", \"sashimi\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"say\", \"say\", \"say\", \"say\", \"say\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"scone\", \"scone\", \"scone\", \"server\", \"server\", \"server\", \"server\", \"server\", \"service\", \"service\", \"service\", \"service\", \"service\", \"shawarma\", \"shawarma\", \"shawarma\", \"shawarma\", \"shop\", \"shop\", \"shop\", \"shop\", \"shop\", \"shrimp\", \"shrimp\", \"shrimp\", \"shrimp\", \"shrimp\", \"si\", \"silky\", \"skin\", \"skin\", \"skin\", \"skin\", \"skin\", \"skinny\", \"snooze\", \"snooze\", \"sont\", \"sore\", \"soup\", \"soup\", \"soup\", \"soup\", \"soup\", \"spotless\", \"squid\", \"staff\", \"staff\", \"staff\", \"staff\", \"staff\", \"starbuck\", \"store\", \"store\", \"store\", \"store\", \"store\", \"stretch\", \"super\", \"super\", \"super\", \"super\", \"super\", \"sur\", \"suzi\", \"t\", \"t\", \"t\", \"t\", \"t\", \"table\", \"table\", \"table\", \"table\", \"table\", \"taco\", \"taco\", \"taco\", \"taco\", \"taco\", \"tai\", \"take\", \"take\", \"take\", \"take\", \"take\", \"takoyaki\", \"tamale\", \"tamale\", \"tamale\", \"tamale\", \"tart\", \"tartar\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tech\", \"tech\", \"tech\", \"tech\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"teller\", \"tempura\", \"tempura\", \"tempura\", \"tempura\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"theater\", \"theater\", \"theater\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thread\", \"thread\", \"thread\", \"tilapia\", \"tile\", \"tile\", \"tim\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tire\", \"tire\", \"tire\", \"tire\", \"tire\", \"topping\", \"topping\", \"topping\", \"topping\", \"topping\", \"tostada\", \"tostada\", \"tr\", \"tr\", \"tr\", \"tr\", \"try\", \"try\", \"try\", \"try\", \"try\", \"un\", \"un\", \"un\", \"un\", \"un\", \"une\", \"unorganized\", \"utilize\", \"utilize\", \"utilize\", \"vanilla\", \"vanilla\", \"vanilla\", \"vanilla\", \"vanilla\", \"ve\", \"ve\", \"ve\", \"ve\", \"ve\", \"vegas\", \"vegas\", \"vegas\", \"vegas\", \"vegas\", \"vet\", \"vet\", \"vet\", \"volcano\", \"vous\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"waiter\", \"waiter\", \"waiter\", \"waiter\", \"waiter\", \"waiting\", \"waiting\", \"waiting\", \"waiting\", \"waitress\", \"waitress\", \"waitress\", \"waitress\", \"waitress\", \"want\", \"want\", \"want\", \"want\", \"want\", \"warranty\", \"warranty\", \"warranty\", \"warranty\", \"wear\", \"wear\", \"wear\", \"wear\", \"wear\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wing\", \"wing\", \"wing\", \"wing\", \"wing\", \"work\", \"work\", \"work\", \"work\", \"work\", \"workmanship\", \"wth\", \"y\", \"y\", \"y\", \"y\", \"y\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yoga\", \"yoga\", \"yoga\", \"yuk\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 4, 5, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el440650775098406222080209\", ldavis_el440650775098406222080209_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el440650775098406222080209\", ldavis_el440650775098406222080209_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el440650775098406222080209\", ldavis_el440650775098406222080209_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Use pyLDAvis (or a ploting tool of your choice) to visualize your results \n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, id2word)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f44a26c754500ff0bf585296075bf754",
     "grade": false,
     "grade_id": "cell-bf9e63d9645bba84",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### 3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, there seemt to be a few clear topics in the reviews. Topic 3, for example, seems to focus on doctor's office reviews. It includes key words like \"dr\", \"appointment\", and \"staff.\" It also did not have much overlap with the other topics. Actually, all of the topics seemed to have mostly distinct keywords and didn't have a lot of overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "Complete one of more of these to push your score towards a three: \n",
    "* Incorporate named entity recognition into your analysis\n",
    "* Compare vectorization methods in the classification section\n",
    "* Analyze more (or all) of the yelp dataset - this one is v. hard. \n",
    "* Use a generator object on the reviews file - this would help you with the analyzing the whole dataset.\n",
    "* Incorporate any of the other yelp dataset entities in your analysis (business, users, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
